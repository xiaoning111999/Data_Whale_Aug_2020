{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 特征工程是什么？\n",
    "\n",
    "* 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。\n",
    "* 特征工程是一项工程活动，最大限度地从原始数据中提取特征以供算法和模型使用。\n",
    "* 特征工程包括以下方面：\n",
    "\n",
    "![特征工程脑图](pic7.jpg)\n",
    "<center>Fig. 1 特征工程脑图</center>\n",
    "\n",
    "\n",
    "特征处理是特征工程的核心部分，sklearn提供了较为完整的特征处理方法，包括数据预处理，特征选择，降维等。\n",
    "\n",
    "\n",
    "本文中使用sklearn中的IRIS（鸢尾花）数据集来对特征处理功能进行说明。IRIS数据集由Fisher在1936年整理，包含4个特征（Sepal.Length（花萼长度）、Sepal.Width（花萼宽度）、Petal.Length（花瓣长度）、Petal.Width（花瓣宽度）），特征值都为正浮点数，单位为厘米。目标值为鸢尾花的分类（Iris Setosa（山鸢尾）、Iris Versicolour（杂色鸢尾），Iris Virginica（维吉尼亚鸢尾））。导入IRIS数据集的代码如下：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    " \n",
    "#导入IRIS数据集\n",
    "iris = load_iris()\n",
    " \n",
    "#特征矩阵\n",
    "X = iris.data\n",
    " \n",
    "#目标向量\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 数据预处理\n",
    "\n",
    "通过特征提取，未经处理的特征常常有以下问题：\n",
    "\n",
    "* 不属于同一量纲：即特征的规格不一样，不能够放在一起比较。无量纲化可以解决这一问题。\n",
    "* 信息冗余：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。\n",
    "* 定性特征不能直接使用：某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值，但是这种方式过于灵活，增加了调参的工作。通常使用哑编码的方式将定性特征转换为定量特征：假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用哑编码后的特征可达到非线性的效果。\n",
    "* 存在缺失值：缺失值需要补充。\n",
    "* 信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。\n",
    "　　\n",
    "* **我们使用sklearn中的preproccessing库来进行数据预处理，可以覆盖以上问题的解决方案。** \n",
    "\n",
    "https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html\n",
    "\n",
    "## 2.1 无量纲化\n",
    "\n",
    "无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。\n",
    "\n",
    "\n",
    "### 2.1.1 标准化\n",
    "标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。其公式为$x' = \\frac{x-\\bar{x}}{\\sigma}$\n",
    "\n",
    "class sklearn.preprocessing.StandardScaler(*, copy=True, with_mean=True, with_std=True)\n",
    "* Parameters:\n",
    "    *copy: boolean, optional, default True\n",
    "        *If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned.\n",
    "    * with_mean: boolean, True by default\n",
    "        * If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory.\n",
    "    * with_std: boolean, True by default\n",
    "        * If True, scale the data to unit variance (or equivalently, unit standard deviation).\n",
    "* Attributes\n",
    "    * scale_: ndarray or None, shape (n_features,)\n",
    "        * Per feature relative scaling of the data. This is calculated using np.sqrt(var_). Equal to None when with_std=False.\n",
    "    * mean_: ndarray or None, shape (n_features,)\n",
    "        * The mean value for each feature in the training set. Equal to None when with_mean=False.\n",
    "    * var_: ndarray or None, shape (n_features,)\n",
    "        * The variance for each feature in the training set. Used to compute scale_. Equal to None when with_std=False.\n",
    "    * n_samples_seen_: int or array, shape (n_features,)\n",
    "        * The number of samples processed by the estimator for each feature. If there are not missing samples, the n_samples_seen will be an integer, otherwise it will be an array. Will be reset on new calls to fit, but increments across partial_fit calls.\n",
    "\n",
    "注意： It is possible to disable either centering or scaling by either passing with_mean=False or with_std=False to the constructor of StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.00681170e-01,  1.01900435e+00, -1.34022653e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.14301691e+00, -1.31979479e-01, -1.34022653e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.38535265e+00,  3.28414053e-01, -1.39706395e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.50652052e+00,  9.82172869e-02, -1.28338910e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.02184904e+00,  1.24920112e+00, -1.34022653e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-5.37177559e-01,  1.93979142e+00, -1.16971425e+00,\n",
       "        -1.05217993e+00],\n",
       "       [-1.50652052e+00,  7.88807586e-01, -1.34022653e+00,\n",
       "        -1.18381211e+00],\n",
       "       [-1.02184904e+00,  7.88807586e-01, -1.28338910e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.74885626e+00, -3.62176246e-01, -1.34022653e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.14301691e+00,  9.82172869e-02, -1.28338910e+00,\n",
       "        -1.44707648e+00],\n",
       "       [-5.37177559e-01,  1.47939788e+00, -1.28338910e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.26418478e+00,  7.88807586e-01, -1.22655167e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.26418478e+00, -1.31979479e-01, -1.34022653e+00,\n",
       "        -1.44707648e+00],\n",
       "       [-1.87002413e+00, -1.31979479e-01, -1.51073881e+00,\n",
       "        -1.44707648e+00],\n",
       "       [-5.25060772e-02,  2.16998818e+00, -1.45390138e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.73673948e-01,  3.09077525e+00, -1.28338910e+00,\n",
       "        -1.05217993e+00],\n",
       "       [-5.37177559e-01,  1.93979142e+00, -1.39706395e+00,\n",
       "        -1.05217993e+00],\n",
       "       [-9.00681170e-01,  1.01900435e+00, -1.34022653e+00,\n",
       "        -1.18381211e+00],\n",
       "       [-1.73673948e-01,  1.70959465e+00, -1.16971425e+00,\n",
       "        -1.18381211e+00],\n",
       "       [-9.00681170e-01,  1.70959465e+00, -1.28338910e+00,\n",
       "        -1.18381211e+00],\n",
       "       [-5.37177559e-01,  7.88807586e-01, -1.16971425e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-9.00681170e-01,  1.47939788e+00, -1.28338910e+00,\n",
       "        -1.05217993e+00],\n",
       "       [-1.50652052e+00,  1.24920112e+00, -1.56757623e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-9.00681170e-01,  5.58610819e-01, -1.16971425e+00,\n",
       "        -9.20547742e-01],\n",
       "       [-1.26418478e+00,  7.88807586e-01, -1.05603939e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.02184904e+00, -1.31979479e-01, -1.22655167e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.02184904e+00,  7.88807586e-01, -1.22655167e+00,\n",
       "        -1.05217993e+00],\n",
       "       [-7.79513300e-01,  1.01900435e+00, -1.28338910e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-7.79513300e-01,  7.88807586e-01, -1.34022653e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.38535265e+00,  3.28414053e-01, -1.22655167e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.26418478e+00,  9.82172869e-02, -1.22655167e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-5.37177559e-01,  7.88807586e-01, -1.28338910e+00,\n",
       "        -1.05217993e+00],\n",
       "       [-7.79513300e-01,  2.40018495e+00, -1.28338910e+00,\n",
       "        -1.44707648e+00],\n",
       "       [-4.16009689e-01,  2.63038172e+00, -1.34022653e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.14301691e+00,  9.82172869e-02, -1.28338910e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.02184904e+00,  3.28414053e-01, -1.45390138e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-4.16009689e-01,  1.01900435e+00, -1.39706395e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.14301691e+00,  1.24920112e+00, -1.34022653e+00,\n",
       "        -1.44707648e+00],\n",
       "       [-1.74885626e+00, -1.31979479e-01, -1.39706395e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-9.00681170e-01,  7.88807586e-01, -1.28338910e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.02184904e+00,  1.01900435e+00, -1.39706395e+00,\n",
       "        -1.18381211e+00],\n",
       "       [-1.62768839e+00, -1.74335684e+00, -1.39706395e+00,\n",
       "        -1.18381211e+00],\n",
       "       [-1.74885626e+00,  3.28414053e-01, -1.39706395e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.02184904e+00,  1.01900435e+00, -1.22655167e+00,\n",
       "        -7.88915558e-01],\n",
       "       [-9.00681170e-01,  1.70959465e+00, -1.05603939e+00,\n",
       "        -1.05217993e+00],\n",
       "       [-1.26418478e+00, -1.31979479e-01, -1.34022653e+00,\n",
       "        -1.18381211e+00],\n",
       "       [-9.00681170e-01,  1.70959465e+00, -1.22655167e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.50652052e+00,  3.28414053e-01, -1.34022653e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-6.58345429e-01,  1.47939788e+00, -1.28338910e+00,\n",
       "        -1.31544430e+00],\n",
       "       [-1.02184904e+00,  5.58610819e-01, -1.34022653e+00,\n",
       "        -1.31544430e+00],\n",
       "       [ 1.40150837e+00,  3.28414053e-01,  5.35408562e-01,\n",
       "         2.64141916e-01],\n",
       "       [ 6.74501145e-01,  3.28414053e-01,  4.21733708e-01,\n",
       "         3.95774101e-01],\n",
       "       [ 1.28034050e+00,  9.82172869e-02,  6.49083415e-01,\n",
       "         3.95774101e-01],\n",
       "       [-4.16009689e-01, -1.74335684e+00,  1.37546573e-01,\n",
       "         1.32509732e-01],\n",
       "       [ 7.95669016e-01, -5.92373012e-01,  4.78571135e-01,\n",
       "         3.95774101e-01],\n",
       "       [-1.73673948e-01, -5.92373012e-01,  4.21733708e-01,\n",
       "         1.32509732e-01],\n",
       "       [ 5.53333275e-01,  5.58610819e-01,  5.35408562e-01,\n",
       "         5.27406285e-01],\n",
       "       [-1.14301691e+00, -1.51316008e+00, -2.60315415e-01,\n",
       "        -2.62386821e-01],\n",
       "       [ 9.16836886e-01, -3.62176246e-01,  4.78571135e-01,\n",
       "         1.32509732e-01],\n",
       "       [-7.79513300e-01, -8.22569778e-01,  8.07091462e-02,\n",
       "         2.64141916e-01],\n",
       "       [-1.02184904e+00, -2.43394714e+00, -1.46640561e-01,\n",
       "        -2.62386821e-01],\n",
       "       [ 6.86617933e-02, -1.31979479e-01,  2.51221427e-01,\n",
       "         3.95774101e-01],\n",
       "       [ 1.89829664e-01, -1.97355361e+00,  1.37546573e-01,\n",
       "        -2.62386821e-01],\n",
       "       [ 3.10997534e-01, -3.62176246e-01,  5.35408562e-01,\n",
       "         2.64141916e-01],\n",
       "       [-2.94841818e-01, -3.62176246e-01, -8.98031345e-02,\n",
       "         1.32509732e-01],\n",
       "       [ 1.03800476e+00,  9.82172869e-02,  3.64896281e-01,\n",
       "         2.64141916e-01],\n",
       "       [-2.94841818e-01, -1.31979479e-01,  4.21733708e-01,\n",
       "         3.95774101e-01],\n",
       "       [-5.25060772e-02, -8.22569778e-01,  1.94384000e-01,\n",
       "        -2.62386821e-01],\n",
       "       [ 4.32165405e-01, -1.97355361e+00,  4.21733708e-01,\n",
       "         3.95774101e-01],\n",
       "       [-2.94841818e-01, -1.28296331e+00,  8.07091462e-02,\n",
       "        -1.30754636e-01],\n",
       "       [ 6.86617933e-02,  3.28414053e-01,  5.92245988e-01,\n",
       "         7.90670654e-01],\n",
       "       [ 3.10997534e-01, -5.92373012e-01,  1.37546573e-01,\n",
       "         1.32509732e-01],\n",
       "       [ 5.53333275e-01, -1.28296331e+00,  6.49083415e-01,\n",
       "         3.95774101e-01],\n",
       "       [ 3.10997534e-01, -5.92373012e-01,  5.35408562e-01,\n",
       "         8.77547895e-04],\n",
       "       [ 6.74501145e-01, -3.62176246e-01,  3.08058854e-01,\n",
       "         1.32509732e-01],\n",
       "       [ 9.16836886e-01, -1.31979479e-01,  3.64896281e-01,\n",
       "         2.64141916e-01],\n",
       "       [ 1.15917263e+00, -5.92373012e-01,  5.92245988e-01,\n",
       "         2.64141916e-01],\n",
       "       [ 1.03800476e+00, -1.31979479e-01,  7.05920842e-01,\n",
       "         6.59038469e-01],\n",
       "       [ 1.89829664e-01, -3.62176246e-01,  4.21733708e-01,\n",
       "         3.95774101e-01],\n",
       "       [-1.73673948e-01, -1.05276654e+00, -1.46640561e-01,\n",
       "        -2.62386821e-01],\n",
       "       [-4.16009689e-01, -1.51316008e+00,  2.38717193e-02,\n",
       "        -1.30754636e-01],\n",
       "       [-4.16009689e-01, -1.51316008e+00, -3.29657076e-02,\n",
       "        -2.62386821e-01],\n",
       "       [-5.25060772e-02, -8.22569778e-01,  8.07091462e-02,\n",
       "         8.77547895e-04],\n",
       "       [ 1.89829664e-01, -8.22569778e-01,  7.62758269e-01,\n",
       "         5.27406285e-01],\n",
       "       [-5.37177559e-01, -1.31979479e-01,  4.21733708e-01,\n",
       "         3.95774101e-01],\n",
       "       [ 1.89829664e-01,  7.88807586e-01,  4.21733708e-01,\n",
       "         5.27406285e-01],\n",
       "       [ 1.03800476e+00,  9.82172869e-02,  5.35408562e-01,\n",
       "         3.95774101e-01],\n",
       "       [ 5.53333275e-01, -1.74335684e+00,  3.64896281e-01,\n",
       "         1.32509732e-01],\n",
       "       [-2.94841818e-01, -1.31979479e-01,  1.94384000e-01,\n",
       "         1.32509732e-01],\n",
       "       [-4.16009689e-01, -1.28296331e+00,  1.37546573e-01,\n",
       "         1.32509732e-01],\n",
       "       [-4.16009689e-01, -1.05276654e+00,  3.64896281e-01,\n",
       "         8.77547895e-04],\n",
       "       [ 3.10997534e-01, -1.31979479e-01,  4.78571135e-01,\n",
       "         2.64141916e-01],\n",
       "       [-5.25060772e-02, -1.05276654e+00,  1.37546573e-01,\n",
       "         8.77547895e-04],\n",
       "       [-1.02184904e+00, -1.74335684e+00, -2.60315415e-01,\n",
       "        -2.62386821e-01],\n",
       "       [-2.94841818e-01, -8.22569778e-01,  2.51221427e-01,\n",
       "         1.32509732e-01],\n",
       "       [-1.73673948e-01, -1.31979479e-01,  2.51221427e-01,\n",
       "         8.77547895e-04],\n",
       "       [-1.73673948e-01, -3.62176246e-01,  2.51221427e-01,\n",
       "         1.32509732e-01],\n",
       "       [ 4.32165405e-01, -3.62176246e-01,  3.08058854e-01,\n",
       "         1.32509732e-01],\n",
       "       [-9.00681170e-01, -1.28296331e+00, -4.30827696e-01,\n",
       "        -1.30754636e-01],\n",
       "       [-1.73673948e-01, -5.92373012e-01,  1.94384000e-01,\n",
       "         1.32509732e-01],\n",
       "       [ 5.53333275e-01,  5.58610819e-01,  1.27429511e+00,\n",
       "         1.71209594e+00],\n",
       "       [-5.25060772e-02, -8.22569778e-01,  7.62758269e-01,\n",
       "         9.22302838e-01],\n",
       "       [ 1.52267624e+00, -1.31979479e-01,  1.21745768e+00,\n",
       "         1.18556721e+00],\n",
       "       [ 5.53333275e-01, -3.62176246e-01,  1.04694540e+00,\n",
       "         7.90670654e-01],\n",
       "       [ 7.95669016e-01, -1.31979479e-01,  1.16062026e+00,\n",
       "         1.31719939e+00],\n",
       "       [ 2.12851559e+00, -1.31979479e-01,  1.61531967e+00,\n",
       "         1.18556721e+00],\n",
       "       [-1.14301691e+00, -1.28296331e+00,  4.21733708e-01,\n",
       "         6.59038469e-01],\n",
       "       [ 1.76501198e+00, -3.62176246e-01,  1.44480739e+00,\n",
       "         7.90670654e-01],\n",
       "       [ 1.03800476e+00, -1.28296331e+00,  1.16062026e+00,\n",
       "         7.90670654e-01],\n",
       "       [ 1.64384411e+00,  1.24920112e+00,  1.33113254e+00,\n",
       "         1.71209594e+00],\n",
       "       [ 7.95669016e-01,  3.28414053e-01,  7.62758269e-01,\n",
       "         1.05393502e+00],\n",
       "       [ 6.74501145e-01, -8.22569778e-01,  8.76433123e-01,\n",
       "         9.22302838e-01],\n",
       "       [ 1.15917263e+00, -1.31979479e-01,  9.90107977e-01,\n",
       "         1.18556721e+00],\n",
       "       [-1.73673948e-01, -1.28296331e+00,  7.05920842e-01,\n",
       "         1.05393502e+00],\n",
       "       [-5.25060772e-02, -5.92373012e-01,  7.62758269e-01,\n",
       "         1.58046376e+00],\n",
       "       [ 6.74501145e-01,  3.28414053e-01,  8.76433123e-01,\n",
       "         1.44883158e+00],\n",
       "       [ 7.95669016e-01, -1.31979479e-01,  9.90107977e-01,\n",
       "         7.90670654e-01],\n",
       "       [ 2.24968346e+00,  1.70959465e+00,  1.67215710e+00,\n",
       "         1.31719939e+00],\n",
       "       [ 2.24968346e+00, -1.05276654e+00,  1.78583195e+00,\n",
       "         1.44883158e+00],\n",
       "       [ 1.89829664e-01, -1.97355361e+00,  7.05920842e-01,\n",
       "         3.95774101e-01],\n",
       "       [ 1.28034050e+00,  3.28414053e-01,  1.10378283e+00,\n",
       "         1.44883158e+00],\n",
       "       [-2.94841818e-01, -5.92373012e-01,  6.49083415e-01,\n",
       "         1.05393502e+00],\n",
       "       [ 2.24968346e+00, -5.92373012e-01,  1.67215710e+00,\n",
       "         1.05393502e+00],\n",
       "       [ 5.53333275e-01, -8.22569778e-01,  6.49083415e-01,\n",
       "         7.90670654e-01],\n",
       "       [ 1.03800476e+00,  5.58610819e-01,  1.10378283e+00,\n",
       "         1.18556721e+00],\n",
       "       [ 1.64384411e+00,  3.28414053e-01,  1.27429511e+00,\n",
       "         7.90670654e-01],\n",
       "       [ 4.32165405e-01, -5.92373012e-01,  5.92245988e-01,\n",
       "         7.90670654e-01],\n",
       "       [ 3.10997534e-01, -1.31979479e-01,  6.49083415e-01,\n",
       "         7.90670654e-01],\n",
       "       [ 6.74501145e-01, -5.92373012e-01,  1.04694540e+00,\n",
       "         1.18556721e+00],\n",
       "       [ 1.64384411e+00, -1.31979479e-01,  1.16062026e+00,\n",
       "         5.27406285e-01],\n",
       "       [ 1.88617985e+00, -5.92373012e-01,  1.33113254e+00,\n",
       "         9.22302838e-01],\n",
       "       [ 2.49201920e+00,  1.70959465e+00,  1.50164482e+00,\n",
       "         1.05393502e+00],\n",
       "       [ 6.74501145e-01, -5.92373012e-01,  1.04694540e+00,\n",
       "         1.31719939e+00],\n",
       "       [ 5.53333275e-01, -5.92373012e-01,  7.62758269e-01,\n",
       "         3.95774101e-01],\n",
       "       [ 3.10997534e-01, -1.05276654e+00,  1.04694540e+00,\n",
       "         2.64141916e-01],\n",
       "       [ 2.24968346e+00, -1.31979479e-01,  1.33113254e+00,\n",
       "         1.44883158e+00],\n",
       "       [ 5.53333275e-01,  7.88807586e-01,  1.04694540e+00,\n",
       "         1.58046376e+00],\n",
       "       [ 6.74501145e-01,  9.82172869e-02,  9.90107977e-01,\n",
       "         7.90670654e-01],\n",
       "       [ 1.89829664e-01, -1.31979479e-01,  5.92245988e-01,\n",
       "         7.90670654e-01],\n",
       "       [ 1.28034050e+00,  9.82172869e-02,  9.33270550e-01,\n",
       "         1.18556721e+00],\n",
       "       [ 1.03800476e+00,  9.82172869e-02,  1.04694540e+00,\n",
       "         1.58046376e+00],\n",
       "       [ 1.28034050e+00,  9.82172869e-02,  7.62758269e-01,\n",
       "         1.44883158e+00],\n",
       "       [-5.25060772e-02, -8.22569778e-01,  7.62758269e-01,\n",
       "         9.22302838e-01],\n",
       "       [ 1.15917263e+00,  3.28414053e-01,  1.21745768e+00,\n",
       "         1.44883158e+00],\n",
       "       [ 1.03800476e+00,  5.58610819e-01,  1.10378283e+00,\n",
       "         1.71209594e+00],\n",
       "       [ 1.03800476e+00, -1.31979479e-01,  8.19595696e-01,\n",
       "         1.44883158e+00],\n",
       "       [ 5.53333275e-01, -1.28296331e+00,  7.05920842e-01,\n",
       "         9.22302838e-01],\n",
       "       [ 7.95669016e-01, -1.31979479e-01,  8.19595696e-01,\n",
       "         1.05393502e+00],\n",
       "       [ 4.32165405e-01,  7.88807586e-01,  9.33270550e-01,\n",
       "         1.44883158e+00],\n",
       "       [ 6.86617933e-02, -1.31979479e-01,  7.62758269e-01,\n",
       "         7.90670654e-01]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 区间缩放法\n",
    "区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。其公式为 $x' = \\frac{x-min}{max-min}$\n",
    "\n",
    "class sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), *, copy=True)[source]\n",
    "\n",
    "* Parameters:\n",
    "    * feature_range: tuple (min, max), default=(0, 1). \n",
    "        * Desired range of transformed data.\n",
    "    * copybool, default=True. \n",
    "        * Set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array).\n",
    "\n",
    "* Attributes:\n",
    "    * min_: ndarray of shape (n_features,). \n",
    "        * Per feature adjustment for minimum. Equivalent to min - X.min(axis=0) * self.scale_\n",
    "    * scale_: ndarray of shape (n_features,)\n",
    "        * Per feature relative scaling of the data. Equivalent to (max - min) / (X.max(axis=0) - X.min(axis=0))\n",
    "    * data_min_: ndarray of shape (n_features,)\n",
    "        * Per feature minimum seen in the data\n",
    "    * data_max_: ndarray of shape (n_features,)\n",
    "        * Per feature maximum seen in the data\n",
    "    * data_range_: ndarray of shape (n_features,)\n",
    "        * Per feature range (data_max_ - data_min_) seen in the data\n",
    "    * n_samples_seen_: int\n",
    "        * The number of samples processed by the estimator. It will be reset on new calls to fit, but increments across partial_fit calls.\n",
    "\n",
    "\n",
    "class sklearn.preprocessing.MaxAbsScaler(*, copy=True) is similar to MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22222222, 0.625     , 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.41666667, 0.06779661, 0.04166667],\n",
       "       [0.11111111, 0.5       , 0.05084746, 0.04166667],\n",
       "       [0.08333333, 0.45833333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.66666667, 0.06779661, 0.04166667],\n",
       "       [0.30555556, 0.79166667, 0.11864407, 0.125     ],\n",
       "       [0.08333333, 0.58333333, 0.06779661, 0.08333333],\n",
       "       [0.19444444, 0.58333333, 0.08474576, 0.04166667],\n",
       "       [0.02777778, 0.375     , 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.45833333, 0.08474576, 0.        ],\n",
       "       [0.30555556, 0.70833333, 0.08474576, 0.04166667],\n",
       "       [0.13888889, 0.58333333, 0.10169492, 0.04166667],\n",
       "       [0.13888889, 0.41666667, 0.06779661, 0.        ],\n",
       "       [0.        , 0.41666667, 0.01694915, 0.        ],\n",
       "       [0.41666667, 0.83333333, 0.03389831, 0.04166667],\n",
       "       [0.38888889, 1.        , 0.08474576, 0.125     ],\n",
       "       [0.30555556, 0.79166667, 0.05084746, 0.125     ],\n",
       "       [0.22222222, 0.625     , 0.06779661, 0.08333333],\n",
       "       [0.38888889, 0.75      , 0.11864407, 0.08333333],\n",
       "       [0.22222222, 0.75      , 0.08474576, 0.08333333],\n",
       "       [0.30555556, 0.58333333, 0.11864407, 0.04166667],\n",
       "       [0.22222222, 0.70833333, 0.08474576, 0.125     ],\n",
       "       [0.08333333, 0.66666667, 0.        , 0.04166667],\n",
       "       [0.22222222, 0.54166667, 0.11864407, 0.16666667],\n",
       "       [0.13888889, 0.58333333, 0.15254237, 0.04166667],\n",
       "       [0.19444444, 0.41666667, 0.10169492, 0.04166667],\n",
       "       [0.19444444, 0.58333333, 0.10169492, 0.125     ],\n",
       "       [0.25      , 0.625     , 0.08474576, 0.04166667],\n",
       "       [0.25      , 0.58333333, 0.06779661, 0.04166667],\n",
       "       [0.11111111, 0.5       , 0.10169492, 0.04166667],\n",
       "       [0.13888889, 0.45833333, 0.10169492, 0.04166667],\n",
       "       [0.30555556, 0.58333333, 0.08474576, 0.125     ],\n",
       "       [0.25      , 0.875     , 0.08474576, 0.        ],\n",
       "       [0.33333333, 0.91666667, 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.45833333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.5       , 0.03389831, 0.04166667],\n",
       "       [0.33333333, 0.625     , 0.05084746, 0.04166667],\n",
       "       [0.16666667, 0.66666667, 0.06779661, 0.        ],\n",
       "       [0.02777778, 0.41666667, 0.05084746, 0.04166667],\n",
       "       [0.22222222, 0.58333333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.625     , 0.05084746, 0.08333333],\n",
       "       [0.05555556, 0.125     , 0.05084746, 0.08333333],\n",
       "       [0.02777778, 0.5       , 0.05084746, 0.04166667],\n",
       "       [0.19444444, 0.625     , 0.10169492, 0.20833333],\n",
       "       [0.22222222, 0.75      , 0.15254237, 0.125     ],\n",
       "       [0.13888889, 0.41666667, 0.06779661, 0.08333333],\n",
       "       [0.22222222, 0.75      , 0.10169492, 0.04166667],\n",
       "       [0.08333333, 0.5       , 0.06779661, 0.04166667],\n",
       "       [0.27777778, 0.70833333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.54166667, 0.06779661, 0.04166667],\n",
       "       [0.75      , 0.5       , 0.62711864, 0.54166667],\n",
       "       [0.58333333, 0.5       , 0.59322034, 0.58333333],\n",
       "       [0.72222222, 0.45833333, 0.66101695, 0.58333333],\n",
       "       [0.33333333, 0.125     , 0.50847458, 0.5       ],\n",
       "       [0.61111111, 0.33333333, 0.61016949, 0.58333333],\n",
       "       [0.38888889, 0.33333333, 0.59322034, 0.5       ],\n",
       "       [0.55555556, 0.54166667, 0.62711864, 0.625     ],\n",
       "       [0.16666667, 0.16666667, 0.38983051, 0.375     ],\n",
       "       [0.63888889, 0.375     , 0.61016949, 0.5       ],\n",
       "       [0.25      , 0.29166667, 0.49152542, 0.54166667],\n",
       "       [0.19444444, 0.        , 0.42372881, 0.375     ],\n",
       "       [0.44444444, 0.41666667, 0.54237288, 0.58333333],\n",
       "       [0.47222222, 0.08333333, 0.50847458, 0.375     ],\n",
       "       [0.5       , 0.375     , 0.62711864, 0.54166667],\n",
       "       [0.36111111, 0.375     , 0.44067797, 0.5       ],\n",
       "       [0.66666667, 0.45833333, 0.57627119, 0.54166667],\n",
       "       [0.36111111, 0.41666667, 0.59322034, 0.58333333],\n",
       "       [0.41666667, 0.29166667, 0.52542373, 0.375     ],\n",
       "       [0.52777778, 0.08333333, 0.59322034, 0.58333333],\n",
       "       [0.36111111, 0.20833333, 0.49152542, 0.41666667],\n",
       "       [0.44444444, 0.5       , 0.6440678 , 0.70833333],\n",
       "       [0.5       , 0.33333333, 0.50847458, 0.5       ],\n",
       "       [0.55555556, 0.20833333, 0.66101695, 0.58333333],\n",
       "       [0.5       , 0.33333333, 0.62711864, 0.45833333],\n",
       "       [0.58333333, 0.375     , 0.55932203, 0.5       ],\n",
       "       [0.63888889, 0.41666667, 0.57627119, 0.54166667],\n",
       "       [0.69444444, 0.33333333, 0.6440678 , 0.54166667],\n",
       "       [0.66666667, 0.41666667, 0.6779661 , 0.66666667],\n",
       "       [0.47222222, 0.375     , 0.59322034, 0.58333333],\n",
       "       [0.38888889, 0.25      , 0.42372881, 0.375     ],\n",
       "       [0.33333333, 0.16666667, 0.47457627, 0.41666667],\n",
       "       [0.33333333, 0.16666667, 0.45762712, 0.375     ],\n",
       "       [0.41666667, 0.29166667, 0.49152542, 0.45833333],\n",
       "       [0.47222222, 0.29166667, 0.69491525, 0.625     ],\n",
       "       [0.30555556, 0.41666667, 0.59322034, 0.58333333],\n",
       "       [0.47222222, 0.58333333, 0.59322034, 0.625     ],\n",
       "       [0.66666667, 0.45833333, 0.62711864, 0.58333333],\n",
       "       [0.55555556, 0.125     , 0.57627119, 0.5       ],\n",
       "       [0.36111111, 0.41666667, 0.52542373, 0.5       ],\n",
       "       [0.33333333, 0.20833333, 0.50847458, 0.5       ],\n",
       "       [0.33333333, 0.25      , 0.57627119, 0.45833333],\n",
       "       [0.5       , 0.41666667, 0.61016949, 0.54166667],\n",
       "       [0.41666667, 0.25      , 0.50847458, 0.45833333],\n",
       "       [0.19444444, 0.125     , 0.38983051, 0.375     ],\n",
       "       [0.36111111, 0.29166667, 0.54237288, 0.5       ],\n",
       "       [0.38888889, 0.41666667, 0.54237288, 0.45833333],\n",
       "       [0.38888889, 0.375     , 0.54237288, 0.5       ],\n",
       "       [0.52777778, 0.375     , 0.55932203, 0.5       ],\n",
       "       [0.22222222, 0.20833333, 0.33898305, 0.41666667],\n",
       "       [0.38888889, 0.33333333, 0.52542373, 0.5       ],\n",
       "       [0.55555556, 0.54166667, 0.84745763, 1.        ],\n",
       "       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n",
       "       [0.77777778, 0.41666667, 0.83050847, 0.83333333],\n",
       "       [0.55555556, 0.375     , 0.77966102, 0.70833333],\n",
       "       [0.61111111, 0.41666667, 0.81355932, 0.875     ],\n",
       "       [0.91666667, 0.41666667, 0.94915254, 0.83333333],\n",
       "       [0.16666667, 0.20833333, 0.59322034, 0.66666667],\n",
       "       [0.83333333, 0.375     , 0.89830508, 0.70833333],\n",
       "       [0.66666667, 0.20833333, 0.81355932, 0.70833333],\n",
       "       [0.80555556, 0.66666667, 0.86440678, 1.        ],\n",
       "       [0.61111111, 0.5       , 0.69491525, 0.79166667],\n",
       "       [0.58333333, 0.29166667, 0.72881356, 0.75      ],\n",
       "       [0.69444444, 0.41666667, 0.76271186, 0.83333333],\n",
       "       [0.38888889, 0.20833333, 0.6779661 , 0.79166667],\n",
       "       [0.41666667, 0.33333333, 0.69491525, 0.95833333],\n",
       "       [0.58333333, 0.5       , 0.72881356, 0.91666667],\n",
       "       [0.61111111, 0.41666667, 0.76271186, 0.70833333],\n",
       "       [0.94444444, 0.75      , 0.96610169, 0.875     ],\n",
       "       [0.94444444, 0.25      , 1.        , 0.91666667],\n",
       "       [0.47222222, 0.08333333, 0.6779661 , 0.58333333],\n",
       "       [0.72222222, 0.5       , 0.79661017, 0.91666667],\n",
       "       [0.36111111, 0.33333333, 0.66101695, 0.79166667],\n",
       "       [0.94444444, 0.33333333, 0.96610169, 0.79166667],\n",
       "       [0.55555556, 0.29166667, 0.66101695, 0.70833333],\n",
       "       [0.66666667, 0.54166667, 0.79661017, 0.83333333],\n",
       "       [0.80555556, 0.5       , 0.84745763, 0.70833333],\n",
       "       [0.52777778, 0.33333333, 0.6440678 , 0.70833333],\n",
       "       [0.5       , 0.41666667, 0.66101695, 0.70833333],\n",
       "       [0.58333333, 0.33333333, 0.77966102, 0.83333333],\n",
       "       [0.80555556, 0.41666667, 0.81355932, 0.625     ],\n",
       "       [0.86111111, 0.33333333, 0.86440678, 0.75      ],\n",
       "       [1.        , 0.75      , 0.91525424, 0.79166667],\n",
       "       [0.58333333, 0.33333333, 0.77966102, 0.875     ],\n",
       "       [0.55555556, 0.33333333, 0.69491525, 0.58333333],\n",
       "       [0.5       , 0.25      , 0.77966102, 0.54166667],\n",
       "       [0.94444444, 0.41666667, 0.86440678, 0.91666667],\n",
       "       [0.55555556, 0.58333333, 0.77966102, 0.95833333],\n",
       "       [0.58333333, 0.45833333, 0.76271186, 0.70833333],\n",
       "       [0.47222222, 0.41666667, 0.6440678 , 0.70833333],\n",
       "       [0.72222222, 0.45833333, 0.74576271, 0.83333333],\n",
       "       [0.66666667, 0.45833333, 0.77966102, 0.95833333],\n",
       "       [0.72222222, 0.45833333, 0.69491525, 0.91666667],\n",
       "       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n",
       "       [0.69444444, 0.5       , 0.83050847, 0.91666667],\n",
       "       [0.66666667, 0.54166667, 0.79661017, 1.        ],\n",
       "       [0.66666667, 0.41666667, 0.71186441, 0.91666667],\n",
       "       [0.55555556, 0.20833333, 0.6779661 , 0.75      ],\n",
       "       [0.61111111, 0.41666667, 0.71186441, 0.79166667],\n",
       "       [0.52777778, 0.58333333, 0.74576271, 0.91666667],\n",
       "       [0.44444444, 0.41666667, 0.69491525, 0.70833333]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#区间缩放，返回值为缩放到[0, 1]区间的数据\n",
    "MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 标准化与正则化的区别\n",
    "\n",
    "简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。\n",
    "\n",
    "正则化是依照特征矩阵的行处理数据，拥有统一的标准，也就是说都转化为“单位向量”。\n",
    "\n",
    "正则化的过程是将每个样本缩放到单位范数（每个样本的范数为1），如果后面要使用如二次型（点积）或者其它核方法计算两个样本之间的相似性这个方法会很有用。这使得学习算法不仅能够拟合数据，而且能够使模型的参数权重尽量的小。sklearn.preprocessing.Normalizer的默认方法是norm='l2',即岭回归（ridget），其超参数$\\alpha$决定了你想正则化这个模型的强度。如果$\\alpha=0$那此时的岭回归便变为了线性回归。如果$\\alpha$非常的大，所有的权重最后都接近于零，最后结果将是一条穿过数据平均值的水平直线。\n",
    "\n",
    "l1: lasso:\n",
    "$J(\\theta) = MSE(\\theta) + \\alpha \\sum_{i=1}^{n}|\\theta_{i}|$ \n",
    "\n",
    "l2: ridget: \n",
    "$J(\\theta) = MSE(\\theta) + \\alpha \\frac{1}{2}\\sum_{i=1}^{n}\\theta_{i}^{2}$ \n",
    "\n",
    "\n",
    "与岭回归稍微不一样的Lasso回归的一个重要特征是它倾向于完全消除最不重要的特征的权重（即将它们设置为零），所以也可以拿来筛选单项特征，通常使用\n",
    "clf=sklearn.linear_model.Lasso(alpha=0.1)，同时用clf.coef_来查看每个特征的权重。\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "class sklearn.preprocessing.normalize(X, norm='l2', *, axis=1, copy=True, return_norm=False)\n",
    "* Parameters\n",
    "    * X{array-like, sparse matrix}, shape [n_samples, n_features]\n",
    "        * The data to normalize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.\n",
    "\n",
    "    * norm: ‘l1’, ‘l2’, or ‘max’, optional (‘l2’ by default)\n",
    "        * The norm to use to normalize each non zero sample (or each non-zero feature if axis is 0).\n",
    "\n",
    "    * axis0 or 1, optional (1 by default)\n",
    "        * axis used to normalize the data along. If 1, independently normalize each sample, otherwise (if 0) normalize each feature.\n",
    "    * copy: boolean, optional, default True\n",
    "        * set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR matrix and if axis is 1).\n",
    "    * return_norm: boolean, default False\n",
    "        * whether to return the computed norms\n",
    "* Returns\n",
    "    * X{array-like, sparse matrix}, shape [n_samples, n_features]\n",
    "        * Normalized input X.\n",
    "    * norms: array, shape [n_samples] if axis=1 else [n_features]\n",
    "        * An array of norms along given axis for X. When X is sparse, a NotImplementedError will be raised for norm ‘l1’ or ‘l2’.\n",
    "\n",
    "其他常用数据预处理方法：\n",
    "* Scaling sparse data: MaxAbsScaler and maxabs_scale \n",
    "* Scaling data with outliers:  robust_scale and RobustScaler\n",
    "* Centering kernel matrices:  KernelCenterer \n",
    "\n",
    "非线性数据预处理方法：\n",
    "* Mapping to a Uniform distribution： QuantileTransformer and quantile_transform provide a non-parametric transformation to map the data to a uniform distribution with values between 0 and 1:\n",
    "* Mapping to a Gaussian distribution： PowerTransformer, Yeo-Johnson or Box-Cox\n",
    "\n",
    "---\n",
    "\n",
    "class sklearn.linear_model.Lasso(alpha=1.0, *, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
    "* Parameters: \n",
    "    * alpha: float, default=1.0\n",
    "        * Constant that multiplies the L1 term. Defaults to 1.0. alpha = 0 is equivalent to an ordinary least square, solved by the LinearRegression object. For numerical reasons, using alpha = 0 with the Lasso object is not advised. Given this, you should use the LinearRegression object.\n",
    "    * fit_intercept: bool, default=True\n",
    "        * Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).\n",
    "    * normalize: bool, default=False\n",
    "        * This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit on an estimator with normalize=False.\n",
    "    * precompute: ‘auto’, bool or array-like of shape (n_features, n_features), default=False\n",
    "        * Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument. For sparse input this option is always True to preserve sparsity.\n",
    "    * copy_X: bool, default=True\n",
    "        * If True, X will be copied; else, it may be overwritten.\n",
    "    * max_iter: int, default=1000\n",
    "        * The maximum number of iterations\n",
    "    * tol: float, default=1e-4\n",
    "        * The tolerance for the optimization: if the updates are smaller than tol, the optimization code checks the dual gap for optimality and continues until it is smaller than tol.\n",
    "    * warm_start: bool, default=False\n",
    "        * When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See the Glossary.\n",
    "    * positive: bool, default=False\n",
    "        * When set to True, forces the coefficients to be positive.\n",
    "    * random_state: int, RandomState instance, default=None\n",
    "        * The seed of the pseudo random number generator that selects a random feature to update. Used when selection == ‘random’. Pass an int for reproducible output across multiple function calls. See Glossary.\n",
    "    * selection{‘cyclic’, ‘random’}, default=’cyclic’\n",
    "        * If set to ‘random’, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to ‘random’) often leads to significantly faster convergence especially when tol is higher than 1e-4.\n",
    "\n",
    "* Attributes\n",
    "    * coef_: ndarray of shape (n_features,) or (n_targets, n_features)\n",
    "        * parameter vector (w in the cost function formula)\n",
    "    * sparse_coef_: sparse matrix of shape (n_features, 1) or (n_targets, n_features)\n",
    "        * sparse representation of the fitted coef_\n",
    "    * intercept_: float or ndarray of shape (n_targets,)\n",
    "        * independent term in decision function.\n",
    "    * n_iter_: int or list of int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.80377277, 0.55160877, 0.22064351, 0.0315205 ],\n",
       "       [0.82813287, 0.50702013, 0.23660939, 0.03380134],\n",
       "       [0.80533308, 0.54831188, 0.2227517 , 0.03426949],\n",
       "       [0.80003025, 0.53915082, 0.26087943, 0.03478392],\n",
       "       [0.790965  , 0.5694948 , 0.2214702 , 0.0316386 ],\n",
       "       [0.78417499, 0.5663486 , 0.2468699 , 0.05808704],\n",
       "       [0.78010936, 0.57660257, 0.23742459, 0.0508767 ],\n",
       "       [0.80218492, 0.54548574, 0.24065548, 0.0320874 ],\n",
       "       [0.80642366, 0.5315065 , 0.25658935, 0.03665562],\n",
       "       [0.81803119, 0.51752994, 0.25041771, 0.01669451],\n",
       "       [0.80373519, 0.55070744, 0.22325977, 0.02976797],\n",
       "       [0.786991  , 0.55745196, 0.26233033, 0.03279129],\n",
       "       [0.82307218, 0.51442011, 0.24006272, 0.01714734],\n",
       "       [0.8025126 , 0.55989251, 0.20529392, 0.01866308],\n",
       "       [0.81120865, 0.55945424, 0.16783627, 0.02797271],\n",
       "       [0.77381111, 0.59732787, 0.2036345 , 0.05430253],\n",
       "       [0.79428944, 0.57365349, 0.19121783, 0.05883625],\n",
       "       [0.80327412, 0.55126656, 0.22050662, 0.04725142],\n",
       "       [0.8068282 , 0.53788547, 0.24063297, 0.04246464],\n",
       "       [0.77964883, 0.58091482, 0.22930848, 0.0458617 ],\n",
       "       [0.8173379 , 0.51462016, 0.25731008, 0.03027177],\n",
       "       [0.78591858, 0.57017622, 0.23115252, 0.06164067],\n",
       "       [0.77577075, 0.60712493, 0.16864581, 0.03372916],\n",
       "       [0.80597792, 0.52151512, 0.26865931, 0.07901744],\n",
       "       [0.776114  , 0.54974742, 0.30721179, 0.03233808],\n",
       "       [0.82647451, 0.4958847 , 0.26447184, 0.03305898],\n",
       "       [0.79778206, 0.5424918 , 0.25529026, 0.06382256],\n",
       "       [0.80641965, 0.54278246, 0.23262105, 0.03101614],\n",
       "       [0.81609427, 0.5336001 , 0.21971769, 0.03138824],\n",
       "       [0.79524064, 0.54144043, 0.27072022, 0.03384003],\n",
       "       [0.80846584, 0.52213419, 0.26948861, 0.03368608],\n",
       "       [0.82225028, 0.51771314, 0.22840286, 0.06090743],\n",
       "       [0.76578311, 0.60379053, 0.22089897, 0.0147266 ],\n",
       "       [0.77867447, 0.59462414, 0.19820805, 0.02831544],\n",
       "       [0.81768942, 0.51731371, 0.25031309, 0.03337508],\n",
       "       [0.82512295, 0.52807869, 0.19802951, 0.03300492],\n",
       "       [0.82699754, 0.52627116, 0.19547215, 0.03007264],\n",
       "       [0.78523221, 0.5769053 , 0.22435206, 0.01602515],\n",
       "       [0.80212413, 0.54690282, 0.23699122, 0.03646019],\n",
       "       [0.80779568, 0.53853046, 0.23758697, 0.03167826],\n",
       "       [0.80033301, 0.56023311, 0.20808658, 0.04801998],\n",
       "       [0.86093857, 0.44003527, 0.24871559, 0.0573959 ],\n",
       "       [0.78609038, 0.57170209, 0.23225397, 0.03573138],\n",
       "       [0.78889479, 0.55222635, 0.25244633, 0.09466737],\n",
       "       [0.76693897, 0.57144472, 0.28572236, 0.06015208],\n",
       "       [0.82210585, 0.51381615, 0.23978087, 0.05138162],\n",
       "       [0.77729093, 0.57915795, 0.24385598, 0.030482  ],\n",
       "       [0.79594782, 0.55370283, 0.24224499, 0.03460643],\n",
       "       [0.79837025, 0.55735281, 0.22595384, 0.03012718],\n",
       "       [0.81228363, 0.5361072 , 0.22743942, 0.03249135],\n",
       "       [0.76701103, 0.35063361, 0.51499312, 0.15340221],\n",
       "       [0.74549757, 0.37274878, 0.52417798, 0.17472599],\n",
       "       [0.75519285, 0.33928954, 0.53629637, 0.16417236],\n",
       "       [0.75384916, 0.31524601, 0.54825394, 0.17818253],\n",
       "       [0.7581754 , 0.32659863, 0.5365549 , 0.17496355],\n",
       "       [0.72232962, 0.35482858, 0.57026022, 0.16474184],\n",
       "       [0.72634846, 0.38046824, 0.54187901, 0.18446945],\n",
       "       [0.75916547, 0.37183615, 0.51127471, 0.15493173],\n",
       "       [0.76301853, 0.33526572, 0.53180079, 0.15029153],\n",
       "       [0.72460233, 0.37623583, 0.54345175, 0.19508524],\n",
       "       [0.76923077, 0.30769231, 0.53846154, 0.15384615],\n",
       "       [0.73923462, 0.37588201, 0.52623481, 0.187941  ],\n",
       "       [0.78892752, 0.28927343, 0.52595168, 0.13148792],\n",
       "       [0.73081412, 0.34743622, 0.56308629, 0.16772783],\n",
       "       [0.75911707, 0.3931142 , 0.48800383, 0.17622361],\n",
       "       [0.76945444, 0.35601624, 0.50531337, 0.16078153],\n",
       "       [0.70631892, 0.37838513, 0.5675777 , 0.18919257],\n",
       "       [0.75676497, 0.35228714, 0.53495455, 0.13047672],\n",
       "       [0.76444238, 0.27125375, 0.55483721, 0.18494574],\n",
       "       [0.76185188, 0.34011245, 0.53057542, 0.14964948],\n",
       "       [0.6985796 , 0.37889063, 0.56833595, 0.21312598],\n",
       "       [0.77011854, 0.35349703, 0.50499576, 0.16412362],\n",
       "       [0.74143307, 0.29421947, 0.57667016, 0.17653168],\n",
       "       [0.73659895, 0.33811099, 0.56754345, 0.14490471],\n",
       "       [0.76741698, 0.34773582, 0.51560829, 0.15588157],\n",
       "       [0.76785726, 0.34902603, 0.51190484, 0.16287881],\n",
       "       [0.76467269, 0.31486523, 0.53976896, 0.15743261],\n",
       "       [0.74088576, 0.33173989, 0.55289982, 0.18798594],\n",
       "       [0.73350949, 0.35452959, 0.55013212, 0.18337737],\n",
       "       [0.78667474, 0.35883409, 0.48304589, 0.13801311],\n",
       "       [0.76521855, 0.33391355, 0.52869645, 0.15304371],\n",
       "       [0.77242925, 0.33706004, 0.51963422, 0.14044168],\n",
       "       [0.76434981, 0.35581802, 0.51395936, 0.15814134],\n",
       "       [0.70779525, 0.31850786, 0.60162596, 0.1887454 ],\n",
       "       [0.69333409, 0.38518561, 0.57777841, 0.1925928 ],\n",
       "       [0.71524936, 0.40530797, 0.53643702, 0.19073316],\n",
       "       [0.75457341, 0.34913098, 0.52932761, 0.16893434],\n",
       "       [0.77530021, 0.28304611, 0.54147951, 0.15998258],\n",
       "       [0.72992443, 0.39103094, 0.53440896, 0.16944674],\n",
       "       [0.74714194, 0.33960997, 0.54337595, 0.17659719],\n",
       "       [0.72337118, 0.34195729, 0.57869695, 0.15782644],\n",
       "       [0.73260391, 0.36029701, 0.55245541, 0.1681386 ],\n",
       "       [0.76262994, 0.34186859, 0.52595168, 0.1577855 ],\n",
       "       [0.76986879, 0.35413965, 0.5081134 , 0.15397376],\n",
       "       [0.73544284, 0.35458851, 0.55158213, 0.1707278 ],\n",
       "       [0.73239618, 0.38547167, 0.53966034, 0.15418867],\n",
       "       [0.73446047, 0.37367287, 0.5411814 , 0.16750853],\n",
       "       [0.75728103, 0.3542121 , 0.52521104, 0.15878473],\n",
       "       [0.78258054, 0.38361791, 0.4603415 , 0.16879188],\n",
       "       [0.7431482 , 0.36505526, 0.5345452 , 0.16948994],\n",
       "       [0.65387747, 0.34250725, 0.62274045, 0.25947519],\n",
       "       [0.69052512, 0.32145135, 0.60718588, 0.22620651],\n",
       "       [0.71491405, 0.30207636, 0.59408351, 0.21145345],\n",
       "       [0.69276796, 0.31889319, 0.61579374, 0.1979337 ],\n",
       "       [0.68619022, 0.31670318, 0.61229281, 0.232249  ],\n",
       "       [0.70953708, 0.28008043, 0.61617694, 0.1960563 ],\n",
       "       [0.67054118, 0.34211284, 0.61580312, 0.23263673],\n",
       "       [0.71366557, 0.28351098, 0.61590317, 0.17597233],\n",
       "       [0.71414125, 0.26647062, 0.61821183, 0.19185884],\n",
       "       [0.69198788, 0.34599394, 0.58626751, 0.24027357],\n",
       "       [0.71562645, 0.3523084 , 0.56149152, 0.22019275],\n",
       "       [0.71576546, 0.30196356, 0.59274328, 0.21249287],\n",
       "       [0.71718148, 0.31640359, 0.58007326, 0.22148252],\n",
       "       [0.6925518 , 0.30375079, 0.60750157, 0.24300063],\n",
       "       [0.67767924, 0.32715549, 0.59589036, 0.28041899],\n",
       "       [0.69589887, 0.34794944, 0.57629125, 0.25008866],\n",
       "       [0.70610474, 0.3258945 , 0.59747324, 0.1955367 ],\n",
       "       [0.69299099, 0.34199555, 0.60299216, 0.19799743],\n",
       "       [0.70600618, 0.2383917 , 0.63265489, 0.21088496],\n",
       "       [0.72712585, 0.26661281, 0.60593821, 0.18178146],\n",
       "       [0.70558934, 0.32722984, 0.58287815, 0.23519645],\n",
       "       [0.68307923, 0.34153961, 0.59769433, 0.24395687],\n",
       "       [0.71486543, 0.25995106, 0.62202576, 0.18567933],\n",
       "       [0.73122464, 0.31338199, 0.56873028, 0.20892133],\n",
       "       [0.69595601, 0.3427843 , 0.59208198, 0.21813547],\n",
       "       [0.71529453, 0.31790868, 0.59607878, 0.17882363],\n",
       "       [0.72785195, 0.32870733, 0.56349829, 0.21131186],\n",
       "       [0.71171214, 0.35002236, 0.57170319, 0.21001342],\n",
       "       [0.69594002, 0.30447376, 0.60894751, 0.22835532],\n",
       "       [0.73089855, 0.30454106, 0.58877939, 0.1624219 ],\n",
       "       [0.72766159, 0.27533141, 0.59982915, 0.18683203],\n",
       "       [0.71578999, 0.34430405, 0.5798805 , 0.18121266],\n",
       "       [0.69417747, 0.30370264, 0.60740528, 0.2386235 ],\n",
       "       [0.72366005, 0.32162669, 0.58582004, 0.17230001],\n",
       "       [0.69385414, 0.29574111, 0.63698085, 0.15924521],\n",
       "       [0.73154399, 0.28501714, 0.57953485, 0.21851314],\n",
       "       [0.67017484, 0.36168166, 0.59571097, 0.2553047 ],\n",
       "       [0.69804799, 0.338117  , 0.59988499, 0.196326  ],\n",
       "       [0.71066905, 0.35533453, 0.56853524, 0.21320072],\n",
       "       [0.72415258, 0.32534391, 0.56672811, 0.22039426],\n",
       "       [0.69997037, 0.32386689, 0.58504986, 0.25073566],\n",
       "       [0.73337886, 0.32948905, 0.54206264, 0.24445962],\n",
       "       [0.69052512, 0.32145135, 0.60718588, 0.22620651],\n",
       "       [0.69193502, 0.32561648, 0.60035539, 0.23403685],\n",
       "       [0.68914871, 0.33943145, 0.58629069, 0.25714504],\n",
       "       [0.72155725, 0.32308533, 0.56001458, 0.24769876],\n",
       "       [0.72965359, 0.28954508, 0.57909015, 0.22005426],\n",
       "       [0.71653899, 0.3307103 , 0.57323119, 0.22047353],\n",
       "       [0.67467072, 0.36998072, 0.58761643, 0.25028107],\n",
       "       [0.69025916, 0.35097923, 0.5966647 , 0.21058754]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#岭回归正则化，返回值为正则化后的数据\n",
    "Normalizer().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.34313725, 0.1372549 , 0.01960784],\n",
       "       [0.51578947, 0.31578947, 0.14736842, 0.02105263],\n",
       "       [0.5       , 0.34042553, 0.13829787, 0.0212766 ],\n",
       "       [0.4893617 , 0.32978723, 0.15957447, 0.0212766 ],\n",
       "       [0.49019608, 0.35294118, 0.1372549 , 0.01960784],\n",
       "       [0.47368421, 0.34210526, 0.14912281, 0.03508772],\n",
       "       [0.4742268 , 0.35051546, 0.1443299 , 0.03092784],\n",
       "       [0.4950495 , 0.33663366, 0.14851485, 0.01980198],\n",
       "       [0.49438202, 0.3258427 , 0.15730337, 0.02247191],\n",
       "       [0.51041667, 0.32291667, 0.15625   , 0.01041667],\n",
       "       [0.5       , 0.34259259, 0.13888889, 0.01851852],\n",
       "       [0.48      , 0.34      , 0.16      , 0.02      ],\n",
       "       [0.51612903, 0.32258065, 0.15053763, 0.01075269],\n",
       "       [0.50588235, 0.35294118, 0.12941176, 0.01176471],\n",
       "       [0.51785714, 0.35714286, 0.10714286, 0.01785714],\n",
       "       [0.475     , 0.36666667, 0.125     , 0.03333333],\n",
       "       [0.49090909, 0.35454545, 0.11818182, 0.03636364],\n",
       "       [0.49514563, 0.33980583, 0.13592233, 0.02912621],\n",
       "       [0.49565217, 0.33043478, 0.14782609, 0.02608696],\n",
       "       [0.47663551, 0.35514019, 0.14018692, 0.02803738],\n",
       "       [0.5046729 , 0.31775701, 0.1588785 , 0.01869159],\n",
       "       [0.47663551, 0.34579439, 0.14018692, 0.03738318],\n",
       "       [0.4893617 , 0.38297872, 0.10638298, 0.0212766 ],\n",
       "       [0.48113208, 0.31132075, 0.16037736, 0.04716981],\n",
       "       [0.46601942, 0.33009709, 0.18446602, 0.01941748],\n",
       "       [0.51020408, 0.30612245, 0.16326531, 0.02040816],\n",
       "       [0.48076923, 0.32692308, 0.15384615, 0.03846154],\n",
       "       [0.5       , 0.33653846, 0.14423077, 0.01923077],\n",
       "       [0.50980392, 0.33333333, 0.1372549 , 0.01960784],\n",
       "       [0.48453608, 0.32989691, 0.16494845, 0.02061856],\n",
       "       [0.49484536, 0.31958763, 0.16494845, 0.02061856],\n",
       "       [0.5046729 , 0.31775701, 0.14018692, 0.03738318],\n",
       "       [0.47706422, 0.37614679, 0.13761468, 0.00917431],\n",
       "       [0.48672566, 0.37168142, 0.12389381, 0.01769912],\n",
       "       [0.50515464, 0.31958763, 0.15463918, 0.02061856],\n",
       "       [0.52083333, 0.33333333, 0.125     , 0.02083333],\n",
       "       [0.52380952, 0.33333333, 0.12380952, 0.01904762],\n",
       "       [0.49      , 0.36      , 0.14      , 0.01      ],\n",
       "       [0.49438202, 0.33707865, 0.14606742, 0.02247191],\n",
       "       [0.5       , 0.33333333, 0.14705882, 0.01960784],\n",
       "       [0.4950495 , 0.34653465, 0.12871287, 0.02970297],\n",
       "       [0.53571429, 0.27380952, 0.1547619 , 0.03571429],\n",
       "       [0.48351648, 0.35164835, 0.14285714, 0.02197802],\n",
       "       [0.46728972, 0.3271028 , 0.14953271, 0.05607477],\n",
       "       [0.45535714, 0.33928571, 0.16964286, 0.03571429],\n",
       "       [0.50526316, 0.31578947, 0.14736842, 0.03157895],\n",
       "       [0.47663551, 0.35514019, 0.14953271, 0.01869159],\n",
       "       [0.4893617 , 0.34042553, 0.14893617, 0.0212766 ],\n",
       "       [0.4953271 , 0.34579439, 0.14018692, 0.01869159],\n",
       "       [0.50505051, 0.33333333, 0.14141414, 0.02020202],\n",
       "       [0.42944785, 0.19631902, 0.28834356, 0.08588957],\n",
       "       [0.41025641, 0.20512821, 0.28846154, 0.09615385],\n",
       "       [0.42073171, 0.18902439, 0.29878049, 0.09146341],\n",
       "       [0.41984733, 0.17557252, 0.30534351, 0.09923664],\n",
       "       [0.42207792, 0.18181818, 0.2987013 , 0.0974026 ],\n",
       "       [0.3986014 , 0.1958042 , 0.31468531, 0.09090909],\n",
       "       [0.39622642, 0.20754717, 0.29559748, 0.10062893],\n",
       "       [0.42241379, 0.20689655, 0.28448276, 0.0862069 ],\n",
       "       [0.42857143, 0.18831169, 0.2987013 , 0.08441558],\n",
       "       [0.39393939, 0.20454545, 0.29545455, 0.10606061],\n",
       "       [0.43478261, 0.17391304, 0.30434783, 0.08695652],\n",
       "       [0.40410959, 0.20547945, 0.28767123, 0.10273973],\n",
       "       [0.45454545, 0.16666667, 0.3030303 , 0.07575758],\n",
       "       [0.40397351, 0.19205298, 0.31125828, 0.09271523],\n",
       "       [0.41791045, 0.21641791, 0.26865672, 0.09701493],\n",
       "       [0.42948718, 0.19871795, 0.28205128, 0.08974359],\n",
       "       [0.38356164, 0.20547945, 0.30821918, 0.10273973],\n",
       "       [0.42647059, 0.19852941, 0.30147059, 0.07352941],\n",
       "       [0.43055556, 0.15277778, 0.3125    , 0.10416667],\n",
       "       [0.42748092, 0.19083969, 0.29770992, 0.08396947],\n",
       "       [0.37579618, 0.20382166, 0.30573248, 0.11464968],\n",
       "       [0.42957746, 0.1971831 , 0.28169014, 0.0915493 ],\n",
       "       [0.41447368, 0.16447368, 0.32236842, 0.09868421],\n",
       "       [0.41216216, 0.18918919, 0.31756757, 0.08108108],\n",
       "       [0.4295302 , 0.19463087, 0.2885906 , 0.08724832],\n",
       "       [0.42857143, 0.19480519, 0.28571429, 0.09090909],\n",
       "       [0.43037975, 0.17721519, 0.30379747, 0.08860759],\n",
       "       [0.40853659, 0.18292683, 0.30487805, 0.10365854],\n",
       "       [0.40268456, 0.19463087, 0.30201342, 0.10067114],\n",
       "       [0.4453125 , 0.203125  , 0.2734375 , 0.078125  ],\n",
       "       [0.4296875 , 0.1875    , 0.296875  , 0.0859375 ],\n",
       "       [0.43650794, 0.19047619, 0.29365079, 0.07936508],\n",
       "       [0.42647059, 0.19852941, 0.28676471, 0.08823529],\n",
       "       [0.38961039, 0.17532468, 0.33116883, 0.1038961 ],\n",
       "       [0.375     , 0.20833333, 0.3125    , 0.10416667],\n",
       "       [0.38709677, 0.21935484, 0.29032258, 0.10322581],\n",
       "       [0.41875   , 0.19375   , 0.29375   , 0.09375   ],\n",
       "       [0.44055944, 0.16083916, 0.30769231, 0.09090909],\n",
       "       [0.4       , 0.21428571, 0.29285714, 0.09285714],\n",
       "       [0.41353383, 0.18796992, 0.30075188, 0.09774436],\n",
       "       [0.40145985, 0.18978102, 0.32116788, 0.08759124],\n",
       "       [0.40397351, 0.1986755 , 0.30463576, 0.09271523],\n",
       "       [0.42647059, 0.19117647, 0.29411765, 0.08823529],\n",
       "       [0.43103448, 0.19827586, 0.28448276, 0.0862069 ],\n",
       "       [0.4057971 , 0.19565217, 0.30434783, 0.0942029 ],\n",
       "       [0.40425532, 0.21276596, 0.29787234, 0.08510638],\n",
       "       [0.40425532, 0.20567376, 0.29787234, 0.09219858],\n",
       "       [0.42176871, 0.19727891, 0.29251701, 0.08843537],\n",
       "       [0.43589744, 0.21367521, 0.25641026, 0.09401709],\n",
       "       [0.41007194, 0.20143885, 0.29496403, 0.09352518],\n",
       "       [0.3480663 , 0.18232044, 0.33149171, 0.13812155],\n",
       "       [0.37419355, 0.17419355, 0.32903226, 0.12258065],\n",
       "       [0.39226519, 0.16574586, 0.32596685, 0.1160221 ],\n",
       "       [0.37951807, 0.1746988 , 0.3373494 , 0.10843373],\n",
       "       [0.37142857, 0.17142857, 0.33142857, 0.12571429],\n",
       "       [0.39378238, 0.15544041, 0.34196891, 0.10880829],\n",
       "       [0.36029412, 0.18382353, 0.33088235, 0.125     ],\n",
       "       [0.3989071 , 0.15846995, 0.3442623 , 0.09836066],\n",
       "       [0.39880952, 0.14880952, 0.3452381 , 0.10714286],\n",
       "       [0.37113402, 0.18556701, 0.31443299, 0.12886598],\n",
       "       [0.38690476, 0.19047619, 0.30357143, 0.11904762],\n",
       "       [0.39263804, 0.16564417, 0.32515337, 0.11656442],\n",
       "       [0.3908046 , 0.17241379, 0.31609195, 0.12068966],\n",
       "       [0.375     , 0.16447368, 0.32894737, 0.13157895],\n",
       "       [0.36024845, 0.17391304, 0.31677019, 0.14906832],\n",
       "       [0.37209302, 0.18604651, 0.30813953, 0.13372093],\n",
       "       [0.38690476, 0.17857143, 0.32738095, 0.10714286],\n",
       "       [0.37745098, 0.18627451, 0.32843137, 0.10784314],\n",
       "       [0.39487179, 0.13333333, 0.35384615, 0.11794872],\n",
       "       [0.40816327, 0.14965986, 0.34013605, 0.10204082],\n",
       "       [0.38121547, 0.17679558, 0.31491713, 0.12707182],\n",
       "       [0.36601307, 0.18300654, 0.32026144, 0.13071895],\n",
       "       [0.40104167, 0.14583333, 0.34895833, 0.10416667],\n",
       "       [0.40127389, 0.17197452, 0.31210191, 0.11464968],\n",
       "       [0.37640449, 0.18539326, 0.32022472, 0.11797753],\n",
       "       [0.3956044 , 0.17582418, 0.32967033, 0.0989011 ],\n",
       "       [0.3974359 , 0.17948718, 0.30769231, 0.11538462],\n",
       "       [0.38607595, 0.18987342, 0.31012658, 0.11392405],\n",
       "       [0.37869822, 0.16568047, 0.33136095, 0.12426036],\n",
       "       [0.40909091, 0.17045455, 0.32954545, 0.09090909],\n",
       "       [0.40659341, 0.15384615, 0.33516484, 0.1043956 ],\n",
       "       [0.39303483, 0.18905473, 0.31840796, 0.09950249],\n",
       "       [0.37647059, 0.16470588, 0.32941176, 0.12941176],\n",
       "       [0.40127389, 0.17834395, 0.32484076, 0.0955414 ],\n",
       "       [0.38853503, 0.1656051 , 0.3566879 , 0.08917197],\n",
       "       [0.40314136, 0.15706806, 0.31937173, 0.12041885],\n",
       "       [0.3559322 , 0.1920904 , 0.31638418, 0.13559322],\n",
       "       [0.38095238, 0.18452381, 0.32738095, 0.10714286],\n",
       "       [0.38461538, 0.19230769, 0.30769231, 0.11538462],\n",
       "       [0.39428571, 0.17714286, 0.30857143, 0.12      ],\n",
       "       [0.37640449, 0.1741573 , 0.31460674, 0.13483146],\n",
       "       [0.39655172, 0.17816092, 0.29310345, 0.13218391],\n",
       "       [0.37419355, 0.17419355, 0.32903226, 0.12258065],\n",
       "       [0.37362637, 0.17582418, 0.32417582, 0.12637363],\n",
       "       [0.36813187, 0.18131868, 0.31318681, 0.13736264],\n",
       "       [0.38953488, 0.1744186 , 0.30232558, 0.13372093],\n",
       "       [0.40127389, 0.15923567, 0.31847134, 0.12101911],\n",
       "       [0.38922156, 0.17964072, 0.31137725, 0.11976048],\n",
       "       [0.3583815 , 0.19653179, 0.31213873, 0.13294798],\n",
       "       [0.37341772, 0.18987342, 0.32278481, 0.11392405]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lasso回归正则化\n",
    "Normalizer(norm='l1').fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.         -0.          0.40811896  0.        ]\n",
      "-0.5337110569441175\n"
     ]
    }
   ],
   "source": [
    "# Lasso回归筛选单项特征\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=0.1)\n",
    "clf.fit(X,y)\n",
    "print(clf.coef_)\n",
    "print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 对定量特征二值化\n",
    "\n",
    "定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0\n",
    "\n",
    "---\n",
    "\n",
    "class sklearn.preprocessing.Binarizer(*, threshold=0.0, copy=True)\n",
    "* Parameters:\n",
    "    * threshold: float, optional (0.0 by default)\n",
    "        * Feature values below or equal to this are replaced by 0, above it by 1. Threshold may not be less than 0 for operations on sparse matrices.\n",
    "    *copy: boolean, optional, default True\n",
    "        * set to False to perform inplace binarization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 0., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 0., 1., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "Binarizer(threshold=3).fit_transform(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 对定性特征哑编码\n",
    "\n",
    "　　由于IRIS数据集的特征皆为定量特征，故使用其目标值进行哑编码（实际上是不需要的）。使用preproccessing库的OneHotEncoder类对数据进行哑编码的代码如下：\n",
    "\n",
    "---\n",
    "\n",
    "class sklearn.preprocessing.OneHotEncoder(*, categories='auto', drop=None, sparse=True, dtype=<class 'numpy.float64'>, handle_unknown='error')\n",
    "* Parameters:\n",
    "    * categories‘auto’ or a list of array-like, default=’auto’\n",
    "        * Categories (unique values) per feature:\n",
    "        * ‘auto’ : Determine categories automatically from the training data.\n",
    "        * list : categories[i] holds the categories expected in the ith column. The passed categories should not mix strings and numeric values within a single feature, and should be sorted in case of numeric values.\n",
    "        * The used categories can be found in the categories_ attribute.\n",
    "    * drop{‘first’, ‘if_binary’} or a array-like of shape (n_features,), default=None\n",
    "        * Specifies a methodology to use to drop one of the categories per feature. This is useful in situations where perfectly collinear features cause problems, such as when feeding the resulting data into a neural network or an unregularized regression.\n",
    "        * However, dropping one category breaks the symmetry of the original representation and can therefore induce a bias in downstream models, for instance for penalized linear classification or regression models.\n",
    "        * None : retain all features (the default).\n",
    "        * ‘first’ : drop the first category in each feature. If only one category is present, the feature will be dropped entirely.\n",
    "        * ‘if_binary’ : drop the first category in each feature with two categories. Features with 1 or more than 2 categories are left intact.\n",
    "\n",
    "        * array : drop[i] is the category in feature X[:, i] that should be dropped.\n",
    "\n",
    "    * sparsebool, default=True\n",
    "        *Will return sparse matrix if set True else will return an array.\n",
    "    * dtypenumber type, default=np.float\n",
    "        * Desired dtype of output.\n",
    "    * handle_unknown{‘error’, ‘ignore’}, default=’error’\n",
    "        * Whether to raise an error or ignore if an unknown categorical feature is present during transform (default is to raise). When this parameter is set to ‘ignore’ and an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. In the inverse transform, an unknown category will be denoted as None.\n",
    "\n",
    "* Attributes\n",
    "    * categories_: list of arrays\n",
    "        * The categories of each feature determined during fitting (in order of the features in X and corresponding with the output of transform). This includes the category specified in drop (if any).\n",
    "    * drop_idx_: array of shape (n_features,)\n",
    "        * drop_idx_[i] is the index in categories_[i] of the category to be dropped for each feature.\n",
    "        * drop_idx_[i] = None if no category is to be dropped from the feature with index i, e.g. when drop='if_binary' and the feature isn’t binary.\n",
    "        * drop_idx_ = None if all the transformed features will be retained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 0)\t1.0\n",
      "  (5, 0)\t1.0\n",
      "  (6, 0)\t1.0\n",
      "  (7, 0)\t1.0\n",
      "  (8, 0)\t1.0\n",
      "  (9, 0)\t1.0\n",
      "  (10, 0)\t1.0\n",
      "  (11, 0)\t1.0\n",
      "  (12, 0)\t1.0\n",
      "  (13, 0)\t1.0\n",
      "  (14, 0)\t1.0\n",
      "  (15, 0)\t1.0\n",
      "  (16, 0)\t1.0\n",
      "  (17, 0)\t1.0\n",
      "  (18, 0)\t1.0\n",
      "  (19, 0)\t1.0\n",
      "  (20, 0)\t1.0\n",
      "  (21, 0)\t1.0\n",
      "  (22, 0)\t1.0\n",
      "  (23, 0)\t1.0\n",
      "  (24, 0)\t1.0\n",
      "  :\t:\n",
      "  (125, 2)\t1.0\n",
      "  (126, 2)\t1.0\n",
      "  (127, 2)\t1.0\n",
      "  (128, 2)\t1.0\n",
      "  (129, 2)\t1.0\n",
      "  (130, 2)\t1.0\n",
      "  (131, 2)\t1.0\n",
      "  (132, 2)\t1.0\n",
      "  (133, 2)\t1.0\n",
      "  (134, 2)\t1.0\n",
      "  (135, 2)\t1.0\n",
      "  (136, 2)\t1.0\n",
      "  (137, 2)\t1.0\n",
      "  (138, 2)\t1.0\n",
      "  (139, 2)\t1.0\n",
      "  (140, 2)\t1.0\n",
      "  (141, 2)\t1.0\n",
      "  (142, 2)\t1.0\n",
      "  (143, 2)\t1.0\n",
      "  (144, 2)\t1.0\n",
      "  (145, 2)\t1.0\n",
      "  (146, 2)\t1.0\n",
      "  (147, 2)\t1.0\n",
      "  (148, 2)\t1.0\n",
      "  (149, 2)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    " \n",
    "#哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据\n",
    "print(OneHotEncoder().fit_transform(iris.target.reshape((-1,1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 缺失值计算\n",
    "由于IRIS数据集没有缺失值，故对数据集新增一个样本，4个特征均赋值为NaN，表示数据缺失。使用preproccessing库的SimpleImputer类对数据进行缺失值计算的代码如下：\n",
    "\n",
    "---\n",
    "\n",
    "class sklearn.impute.SimpleImputer(*, missing_values=nan, strategy='mean', fill_value=None, verbose=0, copy=True, add_indicator=False)\n",
    "\n",
    "* Parameters:\n",
    "    * missing_values: number, string, np.nan (default) or None\n",
    "        * The placeholder for the missing values. All occurrences of missing_values will be imputed. For pandas’ dataframes with nullable integer dtypes with missing values, missing_values should be set to np.nan, since pd.NA will be converted to np.nan.\n",
    "    * strategy: string, default=’mean’\n",
    "        * The imputation strategy.\n",
    "        * If “mean”, then replace missing values using the mean along each column. Can only be used with numeric data.\n",
    "        * If “median”, then replace missing values using the median along each column. Can only be used with numeric data.\n",
    "        * If “most_frequent”, then replace missing using the most frequent value along each column. Can be used with strings or numeric data.\n",
    "        * If “constant”, then replace missing values with fill_value. Can be used with strings or numeric data.\n",
    "    * fill_value: string or numerical value, default=None\n",
    "        * When strategy == “constant”, fill_value is used to replace all occurrences of missing_values. If left to the default, fill_value will be 0 when imputing numerical data and “missing_value” for strings or object data types.\n",
    "    * verbose: integer, default=0\n",
    "        * Controls the verbosity of the imputer.\n",
    "    * copy: boolean, default=True\n",
    "        * If True, a copy of X will be created. If False, imputation will be done in-place whenever possible. Note that, in the following cases, a new copy will always be made, even if copy=False:\n",
    "        * If X is not an array of floating values;\n",
    "        * If X is encoded as a CSR matrix;\n",
    "        * If add_indicator=True.\n",
    "\n",
    "    * add_indicator: boolean, default=False\n",
    "        * If True, a MissingIndicator transform will stack onto output of the imputer’s transform. This allows a predictive estimator to account for missingness despite imputation. If a feature has no missing values at fit/train time, the feature won’t appear on the missing indicator even if there are missing values at transform/test time.\n",
    "\n",
    "* Attributes\n",
    "    * statistics_: array of shape (n_features,)\n",
    "    * The imputation fill value for each feature. Computing statistics can result in np.nan values. During transform, features corresponding to np.nan statistics will be discarded.\n",
    "    * indicator_: sklearn.impute.MissingIndicator\n",
    "        * Indicator used to add binary indicators for missing values. None if add_indicator is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.84333333, 3.05733333, 3.758     , 1.19933333],\n",
       "       [5.1       , 3.5       , 1.4       , 0.2       ],\n",
       "       [4.9       , 3.        , 1.4       , 0.2       ],\n",
       "       [4.7       , 3.2       , 1.3       , 0.2       ],\n",
       "       [4.6       , 3.1       , 1.5       , 0.2       ],\n",
       "       [5.        , 3.6       , 1.4       , 0.2       ],\n",
       "       [5.4       , 3.9       , 1.7       , 0.4       ],\n",
       "       [4.6       , 3.4       , 1.4       , 0.3       ],\n",
       "       [5.        , 3.4       , 1.5       , 0.2       ],\n",
       "       [4.4       , 2.9       , 1.4       , 0.2       ],\n",
       "       [4.9       , 3.1       , 1.5       , 0.1       ],\n",
       "       [5.4       , 3.7       , 1.5       , 0.2       ],\n",
       "       [4.8       , 3.4       , 1.6       , 0.2       ],\n",
       "       [4.8       , 3.        , 1.4       , 0.1       ],\n",
       "       [4.3       , 3.        , 1.1       , 0.1       ],\n",
       "       [5.8       , 4.        , 1.2       , 0.2       ],\n",
       "       [5.7       , 4.4       , 1.5       , 0.4       ],\n",
       "       [5.4       , 3.9       , 1.3       , 0.4       ],\n",
       "       [5.1       , 3.5       , 1.4       , 0.3       ],\n",
       "       [5.7       , 3.8       , 1.7       , 0.3       ],\n",
       "       [5.1       , 3.8       , 1.5       , 0.3       ],\n",
       "       [5.4       , 3.4       , 1.7       , 0.2       ],\n",
       "       [5.1       , 3.7       , 1.5       , 0.4       ],\n",
       "       [4.6       , 3.6       , 1.        , 0.2       ],\n",
       "       [5.1       , 3.3       , 1.7       , 0.5       ],\n",
       "       [4.8       , 3.4       , 1.9       , 0.2       ],\n",
       "       [5.        , 3.        , 1.6       , 0.2       ],\n",
       "       [5.        , 3.4       , 1.6       , 0.4       ],\n",
       "       [5.2       , 3.5       , 1.5       , 0.2       ],\n",
       "       [5.2       , 3.4       , 1.4       , 0.2       ],\n",
       "       [4.7       , 3.2       , 1.6       , 0.2       ],\n",
       "       [4.8       , 3.1       , 1.6       , 0.2       ],\n",
       "       [5.4       , 3.4       , 1.5       , 0.4       ],\n",
       "       [5.2       , 4.1       , 1.5       , 0.1       ],\n",
       "       [5.5       , 4.2       , 1.4       , 0.2       ],\n",
       "       [4.9       , 3.1       , 1.5       , 0.2       ],\n",
       "       [5.        , 3.2       , 1.2       , 0.2       ],\n",
       "       [5.5       , 3.5       , 1.3       , 0.2       ],\n",
       "       [4.9       , 3.6       , 1.4       , 0.1       ],\n",
       "       [4.4       , 3.        , 1.3       , 0.2       ],\n",
       "       [5.1       , 3.4       , 1.5       , 0.2       ],\n",
       "       [5.        , 3.5       , 1.3       , 0.3       ],\n",
       "       [4.5       , 2.3       , 1.3       , 0.3       ],\n",
       "       [4.4       , 3.2       , 1.3       , 0.2       ],\n",
       "       [5.        , 3.5       , 1.6       , 0.6       ],\n",
       "       [5.1       , 3.8       , 1.9       , 0.4       ],\n",
       "       [4.8       , 3.        , 1.4       , 0.3       ],\n",
       "       [5.1       , 3.8       , 1.6       , 0.2       ],\n",
       "       [4.6       , 3.2       , 1.4       , 0.2       ],\n",
       "       [5.3       , 3.7       , 1.5       , 0.2       ],\n",
       "       [5.        , 3.3       , 1.4       , 0.2       ],\n",
       "       [7.        , 3.2       , 4.7       , 1.4       ],\n",
       "       [6.4       , 3.2       , 4.5       , 1.5       ],\n",
       "       [6.9       , 3.1       , 4.9       , 1.5       ],\n",
       "       [5.5       , 2.3       , 4.        , 1.3       ],\n",
       "       [6.5       , 2.8       , 4.6       , 1.5       ],\n",
       "       [5.7       , 2.8       , 4.5       , 1.3       ],\n",
       "       [6.3       , 3.3       , 4.7       , 1.6       ],\n",
       "       [4.9       , 2.4       , 3.3       , 1.        ],\n",
       "       [6.6       , 2.9       , 4.6       , 1.3       ],\n",
       "       [5.2       , 2.7       , 3.9       , 1.4       ],\n",
       "       [5.        , 2.        , 3.5       , 1.        ],\n",
       "       [5.9       , 3.        , 4.2       , 1.5       ],\n",
       "       [6.        , 2.2       , 4.        , 1.        ],\n",
       "       [6.1       , 2.9       , 4.7       , 1.4       ],\n",
       "       [5.6       , 2.9       , 3.6       , 1.3       ],\n",
       "       [6.7       , 3.1       , 4.4       , 1.4       ],\n",
       "       [5.6       , 3.        , 4.5       , 1.5       ],\n",
       "       [5.8       , 2.7       , 4.1       , 1.        ],\n",
       "       [6.2       , 2.2       , 4.5       , 1.5       ],\n",
       "       [5.6       , 2.5       , 3.9       , 1.1       ],\n",
       "       [5.9       , 3.2       , 4.8       , 1.8       ],\n",
       "       [6.1       , 2.8       , 4.        , 1.3       ],\n",
       "       [6.3       , 2.5       , 4.9       , 1.5       ],\n",
       "       [6.1       , 2.8       , 4.7       , 1.2       ],\n",
       "       [6.4       , 2.9       , 4.3       , 1.3       ],\n",
       "       [6.6       , 3.        , 4.4       , 1.4       ],\n",
       "       [6.8       , 2.8       , 4.8       , 1.4       ],\n",
       "       [6.7       , 3.        , 5.        , 1.7       ],\n",
       "       [6.        , 2.9       , 4.5       , 1.5       ],\n",
       "       [5.7       , 2.6       , 3.5       , 1.        ],\n",
       "       [5.5       , 2.4       , 3.8       , 1.1       ],\n",
       "       [5.5       , 2.4       , 3.7       , 1.        ],\n",
       "       [5.8       , 2.7       , 3.9       , 1.2       ],\n",
       "       [6.        , 2.7       , 5.1       , 1.6       ],\n",
       "       [5.4       , 3.        , 4.5       , 1.5       ],\n",
       "       [6.        , 3.4       , 4.5       , 1.6       ],\n",
       "       [6.7       , 3.1       , 4.7       , 1.5       ],\n",
       "       [6.3       , 2.3       , 4.4       , 1.3       ],\n",
       "       [5.6       , 3.        , 4.1       , 1.3       ],\n",
       "       [5.5       , 2.5       , 4.        , 1.3       ],\n",
       "       [5.5       , 2.6       , 4.4       , 1.2       ],\n",
       "       [6.1       , 3.        , 4.6       , 1.4       ],\n",
       "       [5.8       , 2.6       , 4.        , 1.2       ],\n",
       "       [5.        , 2.3       , 3.3       , 1.        ],\n",
       "       [5.6       , 2.7       , 4.2       , 1.3       ],\n",
       "       [5.7       , 3.        , 4.2       , 1.2       ],\n",
       "       [5.7       , 2.9       , 4.2       , 1.3       ],\n",
       "       [6.2       , 2.9       , 4.3       , 1.3       ],\n",
       "       [5.1       , 2.5       , 3.        , 1.1       ],\n",
       "       [5.7       , 2.8       , 4.1       , 1.3       ],\n",
       "       [6.3       , 3.3       , 6.        , 2.5       ],\n",
       "       [5.8       , 2.7       , 5.1       , 1.9       ],\n",
       "       [7.1       , 3.        , 5.9       , 2.1       ],\n",
       "       [6.3       , 2.9       , 5.6       , 1.8       ],\n",
       "       [6.5       , 3.        , 5.8       , 2.2       ],\n",
       "       [7.6       , 3.        , 6.6       , 2.1       ],\n",
       "       [4.9       , 2.5       , 4.5       , 1.7       ],\n",
       "       [7.3       , 2.9       , 6.3       , 1.8       ],\n",
       "       [6.7       , 2.5       , 5.8       , 1.8       ],\n",
       "       [7.2       , 3.6       , 6.1       , 2.5       ],\n",
       "       [6.5       , 3.2       , 5.1       , 2.        ],\n",
       "       [6.4       , 2.7       , 5.3       , 1.9       ],\n",
       "       [6.8       , 3.        , 5.5       , 2.1       ],\n",
       "       [5.7       , 2.5       , 5.        , 2.        ],\n",
       "       [5.8       , 2.8       , 5.1       , 2.4       ],\n",
       "       [6.4       , 3.2       , 5.3       , 2.3       ],\n",
       "       [6.5       , 3.        , 5.5       , 1.8       ],\n",
       "       [7.7       , 3.8       , 6.7       , 2.2       ],\n",
       "       [7.7       , 2.6       , 6.9       , 2.3       ],\n",
       "       [6.        , 2.2       , 5.        , 1.5       ],\n",
       "       [6.9       , 3.2       , 5.7       , 2.3       ],\n",
       "       [5.6       , 2.8       , 4.9       , 2.        ],\n",
       "       [7.7       , 2.8       , 6.7       , 2.        ],\n",
       "       [6.3       , 2.7       , 4.9       , 1.8       ],\n",
       "       [6.7       , 3.3       , 5.7       , 2.1       ],\n",
       "       [7.2       , 3.2       , 6.        , 1.8       ],\n",
       "       [6.2       , 2.8       , 4.8       , 1.8       ],\n",
       "       [6.1       , 3.        , 4.9       , 1.8       ],\n",
       "       [6.4       , 2.8       , 5.6       , 2.1       ],\n",
       "       [7.2       , 3.        , 5.8       , 1.6       ],\n",
       "       [7.4       , 2.8       , 6.1       , 1.9       ],\n",
       "       [7.9       , 3.8       , 6.4       , 2.        ],\n",
       "       [6.4       , 2.8       , 5.6       , 2.2       ],\n",
       "       [6.3       , 2.8       , 5.1       , 1.5       ],\n",
       "       [6.1       , 2.6       , 5.6       , 1.4       ],\n",
       "       [7.7       , 3.        , 6.1       , 2.3       ],\n",
       "       [6.3       , 3.4       , 5.6       , 2.4       ],\n",
       "       [6.4       , 3.1       , 5.5       , 1.8       ],\n",
       "       [6.        , 3.        , 4.8       , 1.8       ],\n",
       "       [6.9       , 3.1       , 5.4       , 2.1       ],\n",
       "       [6.7       , 3.1       , 5.6       , 2.4       ],\n",
       "       [6.9       , 3.1       , 5.1       , 2.3       ],\n",
       "       [5.8       , 2.7       , 5.1       , 1.9       ],\n",
       "       [6.8       , 3.2       , 5.9       , 2.3       ],\n",
       "       [6.7       , 3.3       , 5.7       , 2.5       ],\n",
       "       [6.7       , 3.        , 5.2       , 2.3       ],\n",
       "       [6.3       , 2.5       , 5.        , 1.9       ],\n",
       "       [6.5       , 3.        , 5.2       , 2.        ],\n",
       "       [6.2       , 3.4       , 5.4       , 2.3       ],\n",
       "       [5.9       , 3.        , 5.1       , 1.8       ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import vstack, array, nan\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#缺失值计算，返回值为计算缺失值后的数据\n",
    "#参数missing_value为缺失值的表示形式，默认为NaN\n",
    "#参数strategy为缺失值填充方式，默认为mean（均值）\n",
    "imp = SimpleImputer(missing_values=nan, strategy='mean')\n",
    "X_new = vstack((array([nan, nan, nan, nan]), X))\n",
    "imp.fit_transform(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 数据变换\n",
    "\n",
    "常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  5.1 ,  3.5 , ...,  1.96,  0.28,  0.04],\n",
       "       [ 1.  ,  4.9 ,  3.  , ...,  1.96,  0.28,  0.04],\n",
       "       [ 1.  ,  4.7 ,  3.2 , ...,  1.69,  0.26,  0.04],\n",
       "       ...,\n",
       "       [ 1.  ,  6.5 ,  3.  , ..., 27.04, 10.4 ,  4.  ],\n",
       "       [ 1.  ,  6.2 ,  3.4 , ..., 29.16, 12.42,  5.29],\n",
       "       [ 1.  ,  5.9 ,  3.  , ..., 26.01,  9.18,  3.24]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#多项式转换\n",
    "#参数degree为度，默认值为2\n",
    "PolynomialFeatures().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.80828877, 1.5040774 , 0.87546874, 0.18232156],\n",
       "       [1.77495235, 1.38629436, 0.87546874, 0.18232156],\n",
       "       [1.74046617, 1.43508453, 0.83290912, 0.18232156],\n",
       "       [1.7227666 , 1.41098697, 0.91629073, 0.18232156],\n",
       "       [1.79175947, 1.5260563 , 0.87546874, 0.18232156],\n",
       "       [1.85629799, 1.58923521, 0.99325177, 0.33647224],\n",
       "       [1.7227666 , 1.48160454, 0.87546874, 0.26236426],\n",
       "       [1.79175947, 1.48160454, 0.91629073, 0.18232156],\n",
       "       [1.68639895, 1.36097655, 0.87546874, 0.18232156],\n",
       "       [1.77495235, 1.41098697, 0.91629073, 0.09531018],\n",
       "       [1.85629799, 1.54756251, 0.91629073, 0.18232156],\n",
       "       [1.75785792, 1.48160454, 0.95551145, 0.18232156],\n",
       "       [1.75785792, 1.38629436, 0.87546874, 0.09531018],\n",
       "       [1.66770682, 1.38629436, 0.74193734, 0.09531018],\n",
       "       [1.91692261, 1.60943791, 0.78845736, 0.18232156],\n",
       "       [1.90210753, 1.68639895, 0.91629073, 0.33647224],\n",
       "       [1.85629799, 1.58923521, 0.83290912, 0.33647224],\n",
       "       [1.80828877, 1.5040774 , 0.87546874, 0.26236426],\n",
       "       [1.90210753, 1.56861592, 0.99325177, 0.26236426],\n",
       "       [1.80828877, 1.56861592, 0.91629073, 0.26236426],\n",
       "       [1.85629799, 1.48160454, 0.99325177, 0.18232156],\n",
       "       [1.80828877, 1.54756251, 0.91629073, 0.33647224],\n",
       "       [1.7227666 , 1.5260563 , 0.69314718, 0.18232156],\n",
       "       [1.80828877, 1.45861502, 0.99325177, 0.40546511],\n",
       "       [1.75785792, 1.48160454, 1.06471074, 0.18232156],\n",
       "       [1.79175947, 1.38629436, 0.95551145, 0.18232156],\n",
       "       [1.79175947, 1.48160454, 0.95551145, 0.33647224],\n",
       "       [1.82454929, 1.5040774 , 0.91629073, 0.18232156],\n",
       "       [1.82454929, 1.48160454, 0.87546874, 0.18232156],\n",
       "       [1.74046617, 1.43508453, 0.95551145, 0.18232156],\n",
       "       [1.75785792, 1.41098697, 0.95551145, 0.18232156],\n",
       "       [1.85629799, 1.48160454, 0.91629073, 0.33647224],\n",
       "       [1.82454929, 1.62924054, 0.91629073, 0.09531018],\n",
       "       [1.87180218, 1.64865863, 0.87546874, 0.18232156],\n",
       "       [1.77495235, 1.41098697, 0.91629073, 0.18232156],\n",
       "       [1.79175947, 1.43508453, 0.78845736, 0.18232156],\n",
       "       [1.87180218, 1.5040774 , 0.83290912, 0.18232156],\n",
       "       [1.77495235, 1.5260563 , 0.87546874, 0.09531018],\n",
       "       [1.68639895, 1.38629436, 0.83290912, 0.18232156],\n",
       "       [1.80828877, 1.48160454, 0.91629073, 0.18232156],\n",
       "       [1.79175947, 1.5040774 , 0.83290912, 0.26236426],\n",
       "       [1.70474809, 1.19392247, 0.83290912, 0.26236426],\n",
       "       [1.68639895, 1.43508453, 0.83290912, 0.18232156],\n",
       "       [1.79175947, 1.5040774 , 0.95551145, 0.47000363],\n",
       "       [1.80828877, 1.56861592, 1.06471074, 0.33647224],\n",
       "       [1.75785792, 1.38629436, 0.87546874, 0.26236426],\n",
       "       [1.80828877, 1.56861592, 0.95551145, 0.18232156],\n",
       "       [1.7227666 , 1.43508453, 0.87546874, 0.18232156],\n",
       "       [1.84054963, 1.54756251, 0.91629073, 0.18232156],\n",
       "       [1.79175947, 1.45861502, 0.87546874, 0.18232156],\n",
       "       [2.07944154, 1.43508453, 1.74046617, 0.87546874],\n",
       "       [2.00148   , 1.43508453, 1.70474809, 0.91629073],\n",
       "       [2.06686276, 1.41098697, 1.77495235, 0.91629073],\n",
       "       [1.87180218, 1.19392247, 1.60943791, 0.83290912],\n",
       "       [2.01490302, 1.33500107, 1.7227666 , 0.91629073],\n",
       "       [1.90210753, 1.33500107, 1.70474809, 0.83290912],\n",
       "       [1.98787435, 1.45861502, 1.74046617, 0.95551145],\n",
       "       [1.77495235, 1.22377543, 1.45861502, 0.69314718],\n",
       "       [2.02814825, 1.36097655, 1.7227666 , 0.83290912],\n",
       "       [1.82454929, 1.30833282, 1.58923521, 0.87546874],\n",
       "       [1.79175947, 1.09861229, 1.5040774 , 0.69314718],\n",
       "       [1.93152141, 1.38629436, 1.64865863, 0.91629073],\n",
       "       [1.94591015, 1.16315081, 1.60943791, 0.69314718],\n",
       "       [1.96009478, 1.36097655, 1.74046617, 0.87546874],\n",
       "       [1.88706965, 1.36097655, 1.5260563 , 0.83290912],\n",
       "       [2.04122033, 1.41098697, 1.68639895, 0.87546874],\n",
       "       [1.88706965, 1.38629436, 1.70474809, 0.91629073],\n",
       "       [1.91692261, 1.30833282, 1.62924054, 0.69314718],\n",
       "       [1.97408103, 1.16315081, 1.70474809, 0.91629073],\n",
       "       [1.88706965, 1.25276297, 1.58923521, 0.74193734],\n",
       "       [1.93152141, 1.43508453, 1.75785792, 1.02961942],\n",
       "       [1.96009478, 1.33500107, 1.60943791, 0.83290912],\n",
       "       [1.98787435, 1.25276297, 1.77495235, 0.91629073],\n",
       "       [1.96009478, 1.33500107, 1.74046617, 0.78845736],\n",
       "       [2.00148   , 1.36097655, 1.66770682, 0.83290912],\n",
       "       [2.02814825, 1.38629436, 1.68639895, 0.87546874],\n",
       "       [2.05412373, 1.33500107, 1.75785792, 0.87546874],\n",
       "       [2.04122033, 1.38629436, 1.79175947, 0.99325177],\n",
       "       [1.94591015, 1.36097655, 1.70474809, 0.91629073],\n",
       "       [1.90210753, 1.28093385, 1.5040774 , 0.69314718],\n",
       "       [1.87180218, 1.22377543, 1.56861592, 0.74193734],\n",
       "       [1.87180218, 1.22377543, 1.54756251, 0.69314718],\n",
       "       [1.91692261, 1.30833282, 1.58923521, 0.78845736],\n",
       "       [1.94591015, 1.30833282, 1.80828877, 0.95551145],\n",
       "       [1.85629799, 1.38629436, 1.70474809, 0.91629073],\n",
       "       [1.94591015, 1.48160454, 1.70474809, 0.95551145],\n",
       "       [2.04122033, 1.41098697, 1.74046617, 0.91629073],\n",
       "       [1.98787435, 1.19392247, 1.68639895, 0.83290912],\n",
       "       [1.88706965, 1.38629436, 1.62924054, 0.83290912],\n",
       "       [1.87180218, 1.25276297, 1.60943791, 0.83290912],\n",
       "       [1.87180218, 1.28093385, 1.68639895, 0.78845736],\n",
       "       [1.96009478, 1.38629436, 1.7227666 , 0.87546874],\n",
       "       [1.91692261, 1.28093385, 1.60943791, 0.78845736],\n",
       "       [1.79175947, 1.19392247, 1.45861502, 0.69314718],\n",
       "       [1.88706965, 1.30833282, 1.64865863, 0.83290912],\n",
       "       [1.90210753, 1.38629436, 1.64865863, 0.78845736],\n",
       "       [1.90210753, 1.36097655, 1.64865863, 0.83290912],\n",
       "       [1.97408103, 1.36097655, 1.66770682, 0.83290912],\n",
       "       [1.80828877, 1.25276297, 1.38629436, 0.74193734],\n",
       "       [1.90210753, 1.33500107, 1.62924054, 0.83290912],\n",
       "       [1.98787435, 1.45861502, 1.94591015, 1.25276297],\n",
       "       [1.91692261, 1.30833282, 1.80828877, 1.06471074],\n",
       "       [2.09186406, 1.38629436, 1.93152141, 1.13140211],\n",
       "       [1.98787435, 1.36097655, 1.88706965, 1.02961942],\n",
       "       [2.01490302, 1.38629436, 1.91692261, 1.16315081],\n",
       "       [2.1517622 , 1.38629436, 2.02814825, 1.13140211],\n",
       "       [1.77495235, 1.25276297, 1.70474809, 0.99325177],\n",
       "       [2.11625551, 1.36097655, 1.98787435, 1.02961942],\n",
       "       [2.04122033, 1.25276297, 1.91692261, 1.02961942],\n",
       "       [2.10413415, 1.5260563 , 1.96009478, 1.25276297],\n",
       "       [2.01490302, 1.43508453, 1.80828877, 1.09861229],\n",
       "       [2.00148   , 1.30833282, 1.84054963, 1.06471074],\n",
       "       [2.05412373, 1.38629436, 1.87180218, 1.13140211],\n",
       "       [1.90210753, 1.25276297, 1.79175947, 1.09861229],\n",
       "       [1.91692261, 1.33500107, 1.80828877, 1.22377543],\n",
       "       [2.00148   , 1.43508453, 1.84054963, 1.19392247],\n",
       "       [2.01490302, 1.38629436, 1.87180218, 1.02961942],\n",
       "       [2.16332303, 1.56861592, 2.04122033, 1.16315081],\n",
       "       [2.16332303, 1.28093385, 2.06686276, 1.19392247],\n",
       "       [1.94591015, 1.16315081, 1.79175947, 0.91629073],\n",
       "       [2.06686276, 1.43508453, 1.90210753, 1.19392247],\n",
       "       [1.88706965, 1.33500107, 1.77495235, 1.09861229],\n",
       "       [2.16332303, 1.33500107, 2.04122033, 1.09861229],\n",
       "       [1.98787435, 1.30833282, 1.77495235, 1.02961942],\n",
       "       [2.04122033, 1.45861502, 1.90210753, 1.13140211],\n",
       "       [2.10413415, 1.43508453, 1.94591015, 1.02961942],\n",
       "       [1.97408103, 1.33500107, 1.75785792, 1.02961942],\n",
       "       [1.96009478, 1.38629436, 1.77495235, 1.02961942],\n",
       "       [2.00148   , 1.33500107, 1.88706965, 1.13140211],\n",
       "       [2.10413415, 1.38629436, 1.91692261, 0.95551145],\n",
       "       [2.12823171, 1.33500107, 1.96009478, 1.06471074],\n",
       "       [2.18605128, 1.56861592, 2.00148   , 1.09861229],\n",
       "       [2.00148   , 1.33500107, 1.88706965, 1.16315081],\n",
       "       [1.98787435, 1.33500107, 1.80828877, 0.91629073],\n",
       "       [1.96009478, 1.28093385, 1.88706965, 0.87546874],\n",
       "       [2.16332303, 1.38629436, 1.96009478, 1.19392247],\n",
       "       [1.98787435, 1.48160454, 1.88706965, 1.22377543],\n",
       "       [2.00148   , 1.41098697, 1.87180218, 1.02961942],\n",
       "       [1.94591015, 1.38629436, 1.75785792, 1.02961942],\n",
       "       [2.06686276, 1.41098697, 1.85629799, 1.13140211],\n",
       "       [2.04122033, 1.41098697, 1.88706965, 1.22377543],\n",
       "       [2.06686276, 1.41098697, 1.80828877, 1.19392247],\n",
       "       [1.91692261, 1.30833282, 1.80828877, 1.06471074],\n",
       "       [2.05412373, 1.43508453, 1.93152141, 1.19392247],\n",
       "       [2.04122033, 1.45861502, 1.90210753, 1.25276297],\n",
       "       [2.04122033, 1.38629436, 1.82454929, 1.19392247],\n",
       "       [1.98787435, 1.25276297, 1.79175947, 1.06471074],\n",
       "       [2.01490302, 1.38629436, 1.82454929, 1.09861229],\n",
       "       [1.97408103, 1.48160454, 1.85629799, 1.19392247],\n",
       "       [1.93152141, 1.38629436, 1.80828877, 1.02961942]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import log1p\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    " \n",
    "#自定义转换函数为对数函数的数据变换\n",
    "#第一个参数是单变元函数\n",
    "FunctionTransformer(log1p).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 回顾\n",
    "\n",
    "<center> Table. 1 Sklearn Data Preprocessing Tools </center>\n",
    "\n",
    "|类名\t  | 功能\t| 说明   |\n",
    "|-----  |---------|-----|\n",
    "|StandardScaler\t| 数据预处理（无量纲化） |\t标准化，基于特征矩阵的列，将特征值转换至服从标准正态分布|\n",
    "|MinMaxScaler\t| 数据预处理（无量纲化）\t|区间缩放，基于最大最小值，将特征值转换到[0, 1]区间上|\n",
    "|Normalizer\t| 数据预处理（归一化）\t|基于特征矩阵的行，将样本向量转换为“单位向量”|\n",
    "|Binarizer\t| 数据预处理（二值化）\t|基于给定阈值，将定量特征按阈值划分|\n",
    "|OneHotEncoder\t| 数据预处理（哑编码）\t|将定性数据编码为定量数据|\n",
    "|SimpleImputer\t| 数据预处理（缺失值计算）\t|计算缺失值，缺失值可填充为均值等|\n",
    "|PolynomialFeatures\t| 数据预处理（多项式数据转换）\t| 多项式数据转换|\n",
    "|FunctionTransformer\t| 数据预处理（自定义单元数据转换）|\t使用单变元的函数来转换数据|\n",
    "|VarianceThreshold |\t特征选择（Filter）|\t方差选择法|\n",
    "|SelectKBest |\t特征选择（Filter）|\t可选关联系数、卡方校验、最大信息系数作为得分计算的方法|\n",
    "|RFE\t|特征选择（Wrapper）|\t递归地训练基模型，将权值系数较小的特征从特征集合中消除|\n",
    "|SelectFromModel|\t特征选择（Embedded）|\t训练基模型，选择权值系数较高的特征|\n",
    "|PCA\t|降维（无监督）|\t主成分分析法|\n",
    "|LDA\t|降维（有监督）|\t线性判别分析法|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 特征选择\n",
    "当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：\n",
    "\n",
    "* 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。\n",
    "* 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。\n",
    "\n",
    "---\n",
    "\n",
    "根据特征选择的形式又可以将特征选择方法分为3种：\n",
    "* Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。\n",
    "* Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。\n",
    "* Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。\n",
    "\n",
    "\n",
    "**我们使用sklearn中的feature_selection库来进行特征选择。**\n",
    "https://scikit-learn.org/stable/modules/feature_selection.html#feature-selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 移除Filter\n",
    "\n",
    "### 3.1.1 移除低方差的特征 (Removing features with low variance)\n",
    "假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。如果100%都是1，那这个特征就没意义了。当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用。而且实际当中，一般不太会有95%以上都取某个值的特征存在，所以这种方法虽然简单但是不太好用。可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。\n",
    "\n",
    "class sklearn.feature_selection.VarianceThreshold(threshold=0.0)\n",
    "* Parameters: \n",
    "    * threshold: float, optional\n",
    "        * Features with a training-set variance lower than this threshold will be removed. The default is to keep all features with non-zero variance, i.e. remove the features that have the same value in all samples.\n",
    "* Attributes\n",
    "    * variances_array, shape (n_features,)\n",
    "        *Variances of individual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4],\n",
       "       [1.4],\n",
       "       [1.3],\n",
       "       [1.5],\n",
       "       [1.4],\n",
       "       [1.7],\n",
       "       [1.4],\n",
       "       [1.5],\n",
       "       [1.4],\n",
       "       [1.5],\n",
       "       [1.5],\n",
       "       [1.6],\n",
       "       [1.4],\n",
       "       [1.1],\n",
       "       [1.2],\n",
       "       [1.5],\n",
       "       [1.3],\n",
       "       [1.4],\n",
       "       [1.7],\n",
       "       [1.5],\n",
       "       [1.7],\n",
       "       [1.5],\n",
       "       [1. ],\n",
       "       [1.7],\n",
       "       [1.9],\n",
       "       [1.6],\n",
       "       [1.6],\n",
       "       [1.5],\n",
       "       [1.4],\n",
       "       [1.6],\n",
       "       [1.6],\n",
       "       [1.5],\n",
       "       [1.5],\n",
       "       [1.4],\n",
       "       [1.5],\n",
       "       [1.2],\n",
       "       [1.3],\n",
       "       [1.4],\n",
       "       [1.3],\n",
       "       [1.5],\n",
       "       [1.3],\n",
       "       [1.3],\n",
       "       [1.3],\n",
       "       [1.6],\n",
       "       [1.9],\n",
       "       [1.4],\n",
       "       [1.6],\n",
       "       [1.4],\n",
       "       [1.5],\n",
       "       [1.4],\n",
       "       [4.7],\n",
       "       [4.5],\n",
       "       [4.9],\n",
       "       [4. ],\n",
       "       [4.6],\n",
       "       [4.5],\n",
       "       [4.7],\n",
       "       [3.3],\n",
       "       [4.6],\n",
       "       [3.9],\n",
       "       [3.5],\n",
       "       [4.2],\n",
       "       [4. ],\n",
       "       [4.7],\n",
       "       [3.6],\n",
       "       [4.4],\n",
       "       [4.5],\n",
       "       [4.1],\n",
       "       [4.5],\n",
       "       [3.9],\n",
       "       [4.8],\n",
       "       [4. ],\n",
       "       [4.9],\n",
       "       [4.7],\n",
       "       [4.3],\n",
       "       [4.4],\n",
       "       [4.8],\n",
       "       [5. ],\n",
       "       [4.5],\n",
       "       [3.5],\n",
       "       [3.8],\n",
       "       [3.7],\n",
       "       [3.9],\n",
       "       [5.1],\n",
       "       [4.5],\n",
       "       [4.5],\n",
       "       [4.7],\n",
       "       [4.4],\n",
       "       [4.1],\n",
       "       [4. ],\n",
       "       [4.4],\n",
       "       [4.6],\n",
       "       [4. ],\n",
       "       [3.3],\n",
       "       [4.2],\n",
       "       [4.2],\n",
       "       [4.2],\n",
       "       [4.3],\n",
       "       [3. ],\n",
       "       [4.1],\n",
       "       [6. ],\n",
       "       [5.1],\n",
       "       [5.9],\n",
       "       [5.6],\n",
       "       [5.8],\n",
       "       [6.6],\n",
       "       [4.5],\n",
       "       [6.3],\n",
       "       [5.8],\n",
       "       [6.1],\n",
       "       [5.1],\n",
       "       [5.3],\n",
       "       [5.5],\n",
       "       [5. ],\n",
       "       [5.1],\n",
       "       [5.3],\n",
       "       [5.5],\n",
       "       [6.7],\n",
       "       [6.9],\n",
       "       [5. ],\n",
       "       [5.7],\n",
       "       [4.9],\n",
       "       [6.7],\n",
       "       [4.9],\n",
       "       [5.7],\n",
       "       [6. ],\n",
       "       [4.8],\n",
       "       [4.9],\n",
       "       [5.6],\n",
       "       [5.8],\n",
       "       [6.1],\n",
       "       [6.4],\n",
       "       [5.6],\n",
       "       [5.1],\n",
       "       [5.6],\n",
       "       [6.1],\n",
       "       [5.6],\n",
       "       [5.5],\n",
       "       [4.8],\n",
       "       [5.4],\n",
       "       [5.6],\n",
       "       [5.1],\n",
       "       [5.1],\n",
       "       [5.9],\n",
       "       [5.7],\n",
       "       [5.2],\n",
       "       [5. ],\n",
       "       [5.2],\n",
       "       [5.4],\n",
       "       [5.1]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "#方差选择法，返回值为特征选择后的数据\n",
    "#参数threshold为方差的阈值\n",
    "VarianceThreshold(threshold=3).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 单变量特征选择 (Univariate feature selection)\n",
    "\n",
    "单变量特征选择的原理是分别单独的计算每个变量的某个统计指标，根据该指标来判断哪些指标重要，剔除那些不重要的指标。\n",
    "\n",
    "* 对于分类问题(y离散)，可采用：\n",
    "    * 卡方检验，f_classif, mutual_info_classif，互信息\n",
    "* 对于回归问题(y连续)，可采用：\n",
    "    * 皮尔森相关系数，f_regression, mutual_info_regression，最大信息系数\n",
    "\n",
    "这种方法比较简单，易于运行，易于理解，通常对于理解数据有较好的效果（但对特征优化、提高泛化能力来说不一定有效）。这种方法有许多改进的版本、变种。\n",
    "\n",
    "单变量特征选择基于单变量的统计测试来选择最佳特征。它可以看作预测模型的一项预处理。\n",
    "\n",
    "---\n",
    "    ==Scikit-learn将特征选择程序用包含 transform 函数的对象来展现==：\n",
    "\n",
    "* SelectKBest：\n",
    "    * 移除得分前 k 名以外的所有特征(取top k)\n",
    "* SelectPercentile： \n",
    "    * 移除得分在用户指定百分比以后的特征(取top k%)\n",
    "    * 对每个特征使用通用的单变量统计检验： 假正率(false positive rate) SelectFpr, 伪发现率(false discovery rate) SelectFdr, 或族系误差率 SelectFwe.\n",
    "* GenericUnivariateSelect ：\n",
    "    * 可以设置不同的策略来进行单变量特征选择。同时不同的选择策略也能够使用超参数寻优，从而让我们找到最佳的单变量特征选择策略。\n",
    "　　* 将特征输入到评分函数，返回一个单变量的f_score(F检验的值)或p-values(P值，假设检验中的一个标准，P-value用来和显著性水平作比较)，注意SelectKBest 和 SelectPercentile只有得分，没有p-value。\n",
    "\n",
    "* For classification: chi2, f_classif, mutual_info_classif\n",
    "* For regression: f_regression, mutual_info_regression\n",
    "\n",
    "--- \n",
    "\n",
    "卡方(Chi2)检验\n",
    "\n",
    "经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：\n",
    "$\\chi^{2}=\\sum \\frac{(A-E)^{2}}{E}$\n",
    "\n",
    "---\n",
    "sklearn.feature_selection.chi2(X, y)\n",
    "* Parameters:\n",
    "    * X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "        * Sample vectors.\n",
    "    * y: array-like of shape (n_samples,)\n",
    "        * Target vector (class labels).\n",
    "* Returns:\n",
    "    * chi2: array, shape = (n_features,)\n",
    "        * chi2 statistics of each feature.\n",
    "    * pval: array, shape = (n_features,)\n",
    "        * p-values of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "print(X.shape)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson相关系数 (Pearson Correlation)\n",
    "\n",
    "皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为[-1，1]，-1表示完全的负相关，+1表示完全的正相关，0表示没有线性相关。\n",
    "\n",
    "Pearson Correlation速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的)之后第一时间就执行。\n",
    "\n",
    "---\n",
    "\n",
    "sklearn.feature_selection.f_regression(X, y, *, center=True)\n",
    "* Parameters: \n",
    "    * X: {array-like, sparse matrix} shape = (n_samples, n_features)\n",
    "        * The set of regressors that will be tested sequentially.\n",
    "    * y: array of shape(n_samples).\n",
    "        * The data matrix\n",
    "    * center: True, bool,\n",
    "        * If true, X and y will be centered.\n",
    "\n",
    "* Returns\n",
    "    * F: array, shape=(n_features,)\n",
    "        * F values of features.\n",
    "    * pval: array, shape=(n_features,)\n",
    "        * p-values of F-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 2)\n",
      "[ 233.8389959    32.93720748 1341.93578461 1592.82421036]\n",
      "[2.89047835e-32 5.20156326e-08 4.20187315e-76 4.15531102e-81]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "#选择K个最好的特征，返回选择特征后的数据\n",
    "#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数\n",
    "#参数k为选择的特征个数\n",
    "\n",
    "X_new = SelectKBest(f_regression, k=2).fit_transform(X, y)\n",
    "f_value, p_value = f_regression(X,y)\n",
    "print(X.shape)\n",
    "print(X_new.shape)\n",
    "print(f_value)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他sklearn.feature_selection的方程：\n",
    "* f_classif\n",
    "    * ANOVA F-value between label/feature for classification tasks.\n",
    "* mutual_info_classif\n",
    "    * Mutual information for a discrete target.\n",
    "* chi2\n",
    "    * Chi-squared stats of non-negative features for classification tasks.\n",
    "* f_regression\n",
    "    * F-value between label/feature for regression tasks.\n",
    "* mutual_info_regression\n",
    "    * Mutual information for a continuous target.\n",
    "* SelectPercentile\n",
    "    * Select features based on percentile of the highest scores.\n",
    "* SelectFpr\n",
    "    * Select features based on a false positive rate test.\n",
    "* SelectFdr\n",
    "    * Select features based on an estimated false discovery rate.\n",
    "* SelectFwe\n",
    "    * Select features based on family-wise error rate.\n",
    "* GenericUnivariateSelect\n",
    "    * Univariate feature selector with configurable mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 互信息法\n",
    "\n",
    "互信息和最大信息系数 (Mutual information and maximal information coefficient (MIC)\n",
    "\n",
    "　　经典的互信息（互信息为随机变量X与Y之间的互信息I(X;Y)为单个事件之间互信息的数学期望）也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：\n",
    "\n",
    "互信息直接用于特征选择其实不是太方便：\n",
    "* 它不属于度量方式，也没有办法归一化，在不同数据及上的结果无法做比较；\n",
    "* 对于连续变量的计算不是很方便（X和Y都是集合，x，y都是离散的取值），通常变量需要先离散化，而互信息的结果对离散化的方式很敏感。\n",
    "\n",
    "　　最大信息系数克服了这两个问题。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0，1]。 minepy 提供了MIC功能。\n",
    "\n",
    "反过头来看y=x^2这个例子，MIC算出来的互信息值为1(最大的取值)。\n",
    "MIC的统计能力遭到了 一些质疑 ，当零假设不成立时，MIC的统计就会受到影响。在有的数据集上不存在这个问题，但有的数据集上就存在这个问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11120806  0.82855225 -0.40183082 ... -0.07893167  0.15397029\n",
      "  0.06326656]\n",
      "1.0000000000000009\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from minepy import MINE\n",
    "m = MINE()\n",
    "x = np.random.uniform(-1, 1, 10000)\n",
    "print(x)\n",
    "m.compute_score(x, x**2)\n",
    "print(m.mic())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Wrapper\n",
    "### 3.2.1 递归特征消除 (Recursive Feature Elimination)\n",
    "递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，移除若干权值系数的特征，再基于新的特征集进行下一轮训练。\n",
    "\n",
    "sklearn官方解释：对特征含有权重的预测模型(例如，线性模型对应参数coefficients)，RFE通过递归减少考察的特征集规模来选择特征。首先，预测模型在原始特征上训练，每个特征指定一个权重。之后，那些拥有最小绝对值权重的特征被踢出特征集。如此往复递归，直至剩余的特征数量达到所需的特征数量。\n",
    "\n",
    "RFECV 通过交叉验证的方式执行RFE，以此来选择最佳数量的特征：对于一个数量为d的feature的集合，他的所有的子集的个数是2的d次方减1(包含空集)。指定一个外部的学习算法，比如SVM之类的。通过该算法计算所有子集的validation error。选择error最小的那个子集作为所挑选的特征。\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE\n",
    "\n",
    "示例:\n",
    "Recursive feature elimination: 一个递归特征消除的示例，展示了在数字分类任务中，像素之间的相关性。\n",
    "https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_digits.html#sphx-glr-auto-examples-feature-selection-plot-rfe-digits-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.4, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [1.3, 0.2],\n",
       "       [1.5, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [1.7, 0.4],\n",
       "       [1.4, 0.3],\n",
       "       [1.5, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [1.5, 0.1],\n",
       "       [1.5, 0.2],\n",
       "       [1.6, 0.2],\n",
       "       [1.4, 0.1],\n",
       "       [1.1, 0.1],\n",
       "       [1.2, 0.2],\n",
       "       [1.5, 0.4],\n",
       "       [1.3, 0.4],\n",
       "       [1.4, 0.3],\n",
       "       [1.7, 0.3],\n",
       "       [1.5, 0.3],\n",
       "       [1.7, 0.2],\n",
       "       [1.5, 0.4],\n",
       "       [1. , 0.2],\n",
       "       [1.7, 0.5],\n",
       "       [1.9, 0.2],\n",
       "       [1.6, 0.2],\n",
       "       [1.6, 0.4],\n",
       "       [1.5, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [1.6, 0.2],\n",
       "       [1.6, 0.2],\n",
       "       [1.5, 0.4],\n",
       "       [1.5, 0.1],\n",
       "       [1.4, 0.2],\n",
       "       [1.5, 0.2],\n",
       "       [1.2, 0.2],\n",
       "       [1.3, 0.2],\n",
       "       [1.4, 0.1],\n",
       "       [1.3, 0.2],\n",
       "       [1.5, 0.2],\n",
       "       [1.3, 0.3],\n",
       "       [1.3, 0.3],\n",
       "       [1.3, 0.2],\n",
       "       [1.6, 0.6],\n",
       "       [1.9, 0.4],\n",
       "       [1.4, 0.3],\n",
       "       [1.6, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [1.5, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [4.7, 1.4],\n",
       "       [4.5, 1.5],\n",
       "       [4.9, 1.5],\n",
       "       [4. , 1.3],\n",
       "       [4.6, 1.5],\n",
       "       [4.5, 1.3],\n",
       "       [4.7, 1.6],\n",
       "       [3.3, 1. ],\n",
       "       [4.6, 1.3],\n",
       "       [3.9, 1.4],\n",
       "       [3.5, 1. ],\n",
       "       [4.2, 1.5],\n",
       "       [4. , 1. ],\n",
       "       [4.7, 1.4],\n",
       "       [3.6, 1.3],\n",
       "       [4.4, 1.4],\n",
       "       [4.5, 1.5],\n",
       "       [4.1, 1. ],\n",
       "       [4.5, 1.5],\n",
       "       [3.9, 1.1],\n",
       "       [4.8, 1.8],\n",
       "       [4. , 1.3],\n",
       "       [4.9, 1.5],\n",
       "       [4.7, 1.2],\n",
       "       [4.3, 1.3],\n",
       "       [4.4, 1.4],\n",
       "       [4.8, 1.4],\n",
       "       [5. , 1.7],\n",
       "       [4.5, 1.5],\n",
       "       [3.5, 1. ],\n",
       "       [3.8, 1.1],\n",
       "       [3.7, 1. ],\n",
       "       [3.9, 1.2],\n",
       "       [5.1, 1.6],\n",
       "       [4.5, 1.5],\n",
       "       [4.5, 1.6],\n",
       "       [4.7, 1.5],\n",
       "       [4.4, 1.3],\n",
       "       [4.1, 1.3],\n",
       "       [4. , 1.3],\n",
       "       [4.4, 1.2],\n",
       "       [4.6, 1.4],\n",
       "       [4. , 1.2],\n",
       "       [3.3, 1. ],\n",
       "       [4.2, 1.3],\n",
       "       [4.2, 1.2],\n",
       "       [4.2, 1.3],\n",
       "       [4.3, 1.3],\n",
       "       [3. , 1.1],\n",
       "       [4.1, 1.3],\n",
       "       [6. , 2.5],\n",
       "       [5.1, 1.9],\n",
       "       [5.9, 2.1],\n",
       "       [5.6, 1.8],\n",
       "       [5.8, 2.2],\n",
       "       [6.6, 2.1],\n",
       "       [4.5, 1.7],\n",
       "       [6.3, 1.8],\n",
       "       [5.8, 1.8],\n",
       "       [6.1, 2.5],\n",
       "       [5.1, 2. ],\n",
       "       [5.3, 1.9],\n",
       "       [5.5, 2.1],\n",
       "       [5. , 2. ],\n",
       "       [5.1, 2.4],\n",
       "       [5.3, 2.3],\n",
       "       [5.5, 1.8],\n",
       "       [6.7, 2.2],\n",
       "       [6.9, 2.3],\n",
       "       [5. , 1.5],\n",
       "       [5.7, 2.3],\n",
       "       [4.9, 2. ],\n",
       "       [6.7, 2. ],\n",
       "       [4.9, 1.8],\n",
       "       [5.7, 2.1],\n",
       "       [6. , 1.8],\n",
       "       [4.8, 1.8],\n",
       "       [4.9, 1.8],\n",
       "       [5.6, 2.1],\n",
       "       [5.8, 1.6],\n",
       "       [6.1, 1.9],\n",
       "       [6.4, 2. ],\n",
       "       [5.6, 2.2],\n",
       "       [5.1, 1.5],\n",
       "       [5.6, 1.4],\n",
       "       [6.1, 2.3],\n",
       "       [5.6, 2.4],\n",
       "       [5.5, 1.8],\n",
       "       [4.8, 1.8],\n",
       "       [5.4, 2.1],\n",
       "       [5.6, 2.4],\n",
       "       [5.1, 2.3],\n",
       "       [5.1, 1.9],\n",
       "       [5.9, 2.3],\n",
       "       [5.7, 2.5],\n",
       "       [5.2, 2.3],\n",
       "       [5. , 1.9],\n",
       "       [5.2, 2. ],\n",
       "       [5.4, 2.3],\n",
       "       [5.1, 1.8]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    " \n",
    "#递归特征消除法，返回特征选择后的数据\n",
    "#参数estimator为基模型\n",
    "#参数n_features_to_select为选择的特征个数\n",
    "RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Embedded\n",
    "使用SelectFromModel选择特征 (Feature selection using SelectFromModel)\n",
    "\n",
    "单变量特征选择方法独立的衡量每个特征与响应变量之间的关系，另一种主流的特征选择方法是基于机器学习模型的方法。有些机器学习方法本身就具有对特征进行打分的机制，或者很容易将其运用到特征选择任务中，例如回归模型，SVM，决策树，随机森林等等。其实Pearson相关系数等价于线性回归里的标准化回归系数。\n",
    "\n",
    "SelectFromModel 作为meta-transformer，能够用于拟合后任何拥有coef_或feature_importances_ 属性的预测模型。 如果特征对应的coef_ 或 feature_importances_ 值低于设定的阈值threshold，那么这些特征将被移除。除了手动设置阈值，也可通过字符串参数调用内置的启发式算法(heuristics)来设置阈值，包括：平均值(“mean”), 中位数(“median”)以及他们与浮点数的乘积，如”0.1*mean”。\n",
    "\n",
    "示例: Feature selection using SelectFromModel and LassoCV: 在阈值未知的前提下，选择了Boston dataset中两项最重要的特征。\n",
    "https://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_diabetes.html#sphx-glr-auto-examples-feature-selection-plot-select-from-model-diabetes-py\n",
    "\n",
    "---\n",
    "\n",
    "class sklearn.feature_selection.SelectFromModel(estimator, *, threshold=None, prefit=False, norm_order=1, max_features=None)\n",
    "* Parameters\n",
    "    * estimator: object\n",
    "        * The base estimator from which the transformer is built. This can be both a fitted (if prefit is set to True) or a non-fitted estimator. The estimator must have either a feature_importances_ or coef_ attribute after fitting.\n",
    "    * threshold: string, float, optional default None\n",
    "        * The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If “median” (resp. “mean”), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., “1.25*mean”) may also be used. If None and if the estimator has a parameter penalty set to l1, either explicitly or implicitly (e.g, Lasso), the threshold used is 1e-5. Otherwise, “mean” is used by default.\n",
    "    * prefit: bool, default False\n",
    "        * Whether a prefit model is expected to be passed into the constructor directly or not. If True, transform must be called directly and SelectFromModel cannot be used with cross_val_score, GridSearchCV and similar utilities that clone the estimator. Otherwise train the model using fit and then transform to do feature selection.\n",
    "    * norm_order: non-zero int, inf, -inf, default 1\n",
    "        * Order of the norm used to filter the vectors of coefficients below threshold in the case where the coef_ attribute of the estimator is of dimension 2.\n",
    "    * max_features: int or None, optional\n",
    "        * The maximum number of features to select. To only select based on max_features, set threshold=-np.inf.\n",
    "* Attributes\n",
    "    * estimator_: an estimator\n",
    "        * The base estimator from which the transformer is built. This is stored only when a non-fitted estimator is passed to the SelectFromModel, i.e when prefit is False.\n",
    "    * threshold_: float\n",
    "        * The threshold value used for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 基于惩罚项的特征选择法\n",
    "使用L1范数作为惩罚项的线性模型(Linear models)会得到稀疏解：大部分特征对应的系数为0。当你希望减少特征的维度以用于其它分类器时，可以通过 feature_selection.SelectFromModel 来选择不为0的系数。特别指出，常用于此目的的稀疏预测模型有 linear_model.Lasso（回归）， linear_model.LogisticRegression 和 svm.LinearSVC（分类）。对于SVM和逻辑回归，参数C控制稀疏性：C越小，被选中的特征越少。对于Lasso，参数$\\alpha$越大，被选中的特征越少。\n",
    "\n",
    "Lasso: L1恢复和压缩感知 (L1-recovery and compressive sensing)\n",
    "\n",
    "对于一个好的$\\alpha$值，在满足特定条件下， Lasso 仅使用少量观测值就能够完全恢复出非零的系数。特别地，样本的数量需要“足够大”，否则L1模型的表现会充满随机性，所谓“足够大”取决于非零系数的数量，特征数量的对数，噪声的数量，非零系数的最小绝对值以及设计矩阵X的结构。此外，设计矩阵必须拥有特定的属性，比如不能太过相关(correlated)。 对于非零系数的恢复，还没有一个选择alpha值的通用规则 。alpha值可以通过交叉验证来设置(LassoCV or LassoLarsCV)，尽管这也许会导致模型欠惩罚(under-penalized)：引入少量非相关变量不会影响分数预测。相反BIC (LassoLarsIC) 更倾向于设置较大的alpha值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "print(X.shape)\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 随机稀疏模型 (Randomized sparse models)\n",
    "　　\n",
    "基于L1的稀疏模型的局限在于，当面对一组互相关的特征时，它们只会选择其中一项特征。为了减轻该问题的影响可以使用随机化技术，通过多次重新估计稀疏模型来扰乱设计矩阵_，或通过多次下采样数据来统计一个给定的回归量被选中的次数。\n",
    "\n",
    "==稳定性选择 (Stability Selection)==\n",
    "\n",
    "RandomizedLasso 实现了使用这项策略的Lasso，RandomizedLogisticRegression 使用逻辑回归，适用于分类任务。要得到整个迭代过程的稳定分数，你可以使用 lasso_stability_path。\n",
    "\n",
    "注意到对于非零特征的检测，要使随机稀疏模型比标准F统计量更有效， 那么模型的参考标准需要是稀疏的，换句话说，非零特征应当只占一小部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.3.3 基于树模型的特征选择法 (Tree-based feature selection)\n",
    "基于树的预测模型（见 sklearn.tree 模块，森林见 sklearn.ensemble 模块）能够用来计算特征的重要程度，因此能用来去除不相关的特征（结合 sklearn.feature_selection.SelectFromModel）:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html?highlight=sklearn%20tree#module-sklearn.tree\n",
    "https://scikit-learn.org/stable/modules/classes.html?highlight=klearn%20ensemble#module-sklearn.ensemble\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "[0.09494829 0.05546731 0.39589694 0.45368746]\n",
      "(150, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "print(X.shape)\n",
    "clf = ExtraTreesClassifier().fit(X, y)\n",
    "print(clf.feature_importances_)  \n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "print(X_new.shape)               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 将特征选择过程融入pipeline (Feature selection as part of a pipeline)\n",
    "特征选择常常被当作学习之前的一项预处理。在scikit-learn中推荐使用sklearn.pipeline.Pipeline\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html?highlight=pipeline#sklearn.pipeline.Pipeline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature_selection',\n",
       "                 SelectFromModel(estimator=LinearSVC(dual=False,\n",
       "                                                     penalty='l1'))),\n",
       "                ('classification', RandomForestClassifier())])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False))),\n",
    "  ('classification', RandomForestClassifier())\n",
    "])\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 回顾\n",
    "\n",
    "<center> Table. 2 sklearn特征选择方法 </center>\n",
    "\n",
    "|类|所属方式|说明|\n",
    "|--|-------|----|\n",
    "|VarianceThreshold|\tFilter|\t方差选择法(移除低方差的特征)|\n",
    "|SelectKBest|\tFilter|\t可选关联系数、卡方校验、最大信息系数作为得分计算的方法|\n",
    "|RFE|\tWrapper|\t递归地训练基模型，将权值系数较小的特征从特征集合中消除|\n",
    "|SelectFromModel|\tEmbedded|\t训练基模型，选择权值系数较高的特征|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 降维\n",
    "当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法除了以上提到的基于L1惩罚项的模型以外，另外还有主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。\n",
    "\n",
    "---\n",
    "\n",
    "class sklearn.decomposition.PCA(n_components=None, *, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)\n",
    "* Parameters\n",
    "    * n_components: int, float, None or str\n",
    "        * Number of components to keep. if n_components is not set all components are kept:\n",
    "        * n_components == min(n_samples, n_features)\n",
    "        * If n_components == 'mle' and svd_solver == 'full', Minka’s MLE is used to guess the dimension. Use of n_components == 'mle' will interpret svd_solver == 'auto' as svd_solver == 'full'.\n",
    "        * If 0 < n_components < 1 and svd_solver == 'full', select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components.\n",
    "        * If svd_solver == 'arpack', the number of components must be strictly less than the minimum of n_features and n_samples.\n",
    "\n",
    "        * Hence, the None case results in: n_components == min(n_samples, n_features) - 1\n",
    "    * copy: bool, default=True\n",
    "        * If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead.\n",
    "    * whiten: bool, optional (default False)\n",
    "        * When True (False by default) the components_ vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances.\n",
    "        * Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions.\n",
    "    * svd_solver: str {‘auto’, ‘full’, ‘arpack’, ‘randomized’}\n",
    "        * If auto : The solver is selected by a default policy based on X.shape and n_components: if the input data is larger than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, then the more efficient ‘randomized’ method is enabled. Otherwise the exact full SVD is computed and optionally truncated afterwards.\n",
    "        * If full : run exact full SVD calling the standard LAPACK solver via scipy.linalg.svd and select the components by postprocessing\n",
    "        * If arpack : run SVD truncated to n_components calling ARPACK solver via scipy.sparse.linalg.svds. It requires strictly 0 < n_components < min(X.shape)\n",
    "        * If randomized : run randomized SVD by the method of Halko et al.\n",
    "    * tol: float >= 0, optional (default .0)\n",
    "        * Tolerance for singular values computed by svd_solver == ‘arpack’.\n",
    "    * iterated_power: int >= 0, or ‘auto’, (default ‘auto’)\n",
    "        * Number of iterations for the power method computed by svd_solver == ‘randomized’.\n",
    "    * random_state: int, RandomState instance, default=None\n",
    "        * Used when svd_solver == ‘arpack’ or ‘randomized’. Pass an int for reproducible results across multiple function calls. See Glossary.\n",
    "\n",
    "* Attributes\n",
    "    * components_: array, shape (n_components, n_features)\n",
    "        * Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by explained_variance_.\n",
    "    * explained_variance_: array, shape (n_components,)\n",
    "        * The amount of variance explained by each of the selected components.\n",
    "        * Equal to n_components largest eigenvalues of the covariance matrix of X.\n",
    "    * explained_variance_ratio_: array, shape (n_components,)\n",
    "        * Percentage of variance explained by each of the selected components.\n",
    "        * If n_components is not set then all components are stored and the sum of the ratios is equal to 1.0.\n",
    "\n",
    "    * singular_values_: array, shape (n_components,)\n",
    "        * The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the n_components variables in the lower-dimensional space.\n",
    "    * mean_: array, shape (n_features,)\n",
    "        * Per-feature empirical mean, estimated from the training set.\n",
    "        * Equal to X.mean(axis=0).\n",
    "    * n_components_: int\n",
    "        * The estimated number of components. When n_components is set to ‘mle’ or a number between 0 and 1 (with svd_solver == ‘full’) this number is estimated from input data. Otherwise it equals the parameter n_components, or the lesser value of n_features and n_samples if n_components is None.\n",
    "    * n_features_: int\n",
    "        * Number of features in the training data.\n",
    "    * n_samples_: int\n",
    "        * Number of samples in the training data.\n",
    "    * noise_variance_float\n",
    "        * The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See “Pattern Recognition and Machine Learning” by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf. It is required to compute the estimated data covariance and score samples.\n",
    "\n",
    "        * Equal to the average of (min(n_features, n_samples) - n_components) smallest eigenvalues of the covariance matrix of X.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92461872, 0.05306648])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    " \n",
    "#主成分分析法，返回降维后的数据\n",
    "#参数n_components为主成分数目\n",
    "pca=PCA(n_components=2).fit(X)\n",
    "pca.components_\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[0.92461872 0.05306648 0.01710261 0.00521218]\n"
     ]
    }
   ],
   "source": [
    "# 方法一： 挑选保留95%方差的n_components\n",
    "pca=PCA().fit(X)\n",
    "cumsum=np.cumsum(pca.explained_variance_ratio_)\n",
    "d=np.argmax(cumsum>=0.95)+1\n",
    "print(d)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[0.92461872 0.05306648]\n"
     ]
    }
   ],
   "source": [
    "#方法二： 挑选保留95%方差的n_components\n",
    "pca=PCA(n_components=0.95).fit(X,y)\n",
    "print(pca.n_components_)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.86982412, 0.13017588],\n",
       "       [0.84319344, 0.15680656],\n",
       "       [0.85812854, 0.14187146],\n",
       "       [0.82949911, 0.17050089],\n",
       "       [0.87164823, 0.12835177],\n",
       "       [0.81257228, 0.18742772],\n",
       "       [0.82701706, 0.17298294],\n",
       "       [0.85463678, 0.14536322],\n",
       "       [0.82430237, 0.17569763],\n",
       "       [0.86728628, 0.13271372],\n",
       "       [0.87474477, 0.12525523],\n",
       "       [0.83958899, 0.16041101],\n",
       "       [0.86985932, 0.13014068],\n",
       "       [0.88268152, 0.11731848],\n",
       "       [0.90626008, 0.09373992],\n",
       "       [0.85841079, 0.14158921],\n",
       "       [0.85046775, 0.14953225],\n",
       "       [0.84359055, 0.15640945],\n",
       "       [0.84102201, 0.15897799],\n",
       "       [0.84753953, 0.15246047],\n",
       "       [0.84330927, 0.15669073],\n",
       "       [0.81624926, 0.18375074],\n",
       "       [0.89117484, 0.10882516],\n",
       "       [0.74277017, 0.25722983],\n",
       "       [0.80340545, 0.19659455],\n",
       "       [0.82253252, 0.17746748],\n",
       "       [0.78591754, 0.21408246],\n",
       "       [0.86319719, 0.13680281],\n",
       "       [0.8678587 , 0.1321413 ],\n",
       "       [0.82604362, 0.17395638],\n",
       "       [0.82289465, 0.17710535],\n",
       "       [0.80770508, 0.19229492],\n",
       "       [0.90369997, 0.09630003],\n",
       "       [0.89768217, 0.10231783],\n",
       "       [0.83741493, 0.16258507],\n",
       "       [0.87263421, 0.12736579],\n",
       "       [0.88424842, 0.11575158],\n",
       "       [0.89227569, 0.10772431],\n",
       "       [0.84145376, 0.15854624],\n",
       "       [0.85680817, 0.14319183],\n",
       "       [0.85087365, 0.14912635],\n",
       "       [0.76483636, 0.23516364],\n",
       "       [0.85135395, 0.14864605],\n",
       "       [0.73903905, 0.26096095],\n",
       "       [0.77764755, 0.22235245],\n",
       "       [0.80977387, 0.19022613],\n",
       "       [0.86464016, 0.13535984],\n",
       "       [0.84594641, 0.15405359],\n",
       "       [0.87294544, 0.12705456],\n",
       "       [0.85967194, 0.14032806],\n",
       "       [0.31758296, 0.68241704],\n",
       "       [0.30271923, 0.69728077],\n",
       "       [0.27670721, 0.72329279],\n",
       "       [0.23731423, 0.76268577],\n",
       "       [0.25581146, 0.74418854],\n",
       "       [0.26102325, 0.73897675],\n",
       "       [0.28225468, 0.71774532],\n",
       "       [0.3405343 , 0.6594657 ],\n",
       "       [0.29424656, 0.70575344],\n",
       "       [0.27111758, 0.72888242],\n",
       "       [0.26867374, 0.73132626],\n",
       "       [0.29029157, 0.70970843],\n",
       "       [0.28562004, 0.71437996],\n",
       "       [0.25722239, 0.74277761],\n",
       "       [0.34937026, 0.65062974],\n",
       "       [0.32274811, 0.67725189],\n",
       "       [0.25605422, 0.74394578],\n",
       "       [0.32964829, 0.67035171],\n",
       "       [0.18436047, 0.81563953],\n",
       "       [0.30309623, 0.69690377],\n",
       "       [0.23118919, 0.76881081],\n",
       "       [0.31883459, 0.68116541],\n",
       "       [0.19235878, 0.80764122],\n",
       "       [0.270212  , 0.729788  ],\n",
       "       [0.31334786, 0.68665214],\n",
       "       [0.30918034, 0.69081966],\n",
       "       [0.26137966, 0.73862034],\n",
       "       [0.2308789 , 0.7691211 ],\n",
       "       [0.25828081, 0.74171919],\n",
       "       [0.37550647, 0.62449353],\n",
       "       [0.29668786, 0.70331214],\n",
       "       [0.3221075 , 0.6778925 ],\n",
       "       [0.31998485, 0.68001515],\n",
       "       [0.18084382, 0.81915618],\n",
       "       [0.24951403, 0.75048597],\n",
       "       [0.29834838, 0.70165162],\n",
       "       [0.28608586, 0.71391414],\n",
       "       [0.23153454, 0.76846546],\n",
       "       [0.31436212, 0.68563788],\n",
       "       [0.26217016, 0.73782984],\n",
       "       [0.25172306, 0.74827694],\n",
       "       [0.27610387, 0.72389613],\n",
       "       [0.29886682, 0.70113318],\n",
       "       [0.33172041, 0.66827959],\n",
       "       [0.27177992, 0.72822008],\n",
       "       [0.3222446 , 0.6777554 ],\n",
       "       [0.29802276, 0.70197724],\n",
       "       [0.30645586, 0.69354414],\n",
       "       [0.37756681, 0.62243319],\n",
       "       [0.29569749, 0.70430251],\n",
       "       [0.12300673, 0.87699327],\n",
       "       [0.14855955, 0.85144045],\n",
       "       [0.1482887 , 0.8517113 ],\n",
       "       [0.1597053 , 0.8402947 ],\n",
       "       [0.1320891 , 0.8679109 ],\n",
       "       [0.12410957, 0.87589043],\n",
       "       [0.15851811, 0.84148189],\n",
       "       [0.1451728 , 0.8548272 ],\n",
       "       [0.12160363, 0.87839637],\n",
       "       [0.16259669, 0.83740331],\n",
       "       [0.20843592, 0.79156408],\n",
       "       [0.15233449, 0.84766551],\n",
       "       [0.16334315, 0.83665685],\n",
       "       [0.12584596, 0.87415404],\n",
       "       [0.12110607, 0.87889393],\n",
       "       [0.16700713, 0.83299287],\n",
       "       [0.1811714 , 0.8188286 ],\n",
       "       [0.18214261, 0.81785739],\n",
       "       [0.07953805, 0.92046195],\n",
       "       [0.14366293, 0.85633707],\n",
       "       [0.15703246, 0.84296754],\n",
       "       [0.15659276, 0.84340724],\n",
       "       [0.11215467, 0.88784533],\n",
       "       [0.18407485, 0.81592515],\n",
       "       [0.17717816, 0.82282184],\n",
       "       [0.1886479 , 0.8113521 ],\n",
       "       [0.19863589, 0.80136411],\n",
       "       [0.2096191 , 0.7903809 ],\n",
       "       [0.12978736, 0.87021264],\n",
       "       [0.20014347, 0.79985653],\n",
       "       [0.14172948, 0.85827052],\n",
       "       [0.2201539 , 0.7798461 ],\n",
       "       [0.12303292, 0.87696708],\n",
       "       [0.21091995, 0.78908005],\n",
       "       [0.15882564, 0.84117436],\n",
       "       [0.13723668, 0.86276332],\n",
       "       [0.15798491, 0.84201509],\n",
       "       [0.18833092, 0.81166908],\n",
       "       [0.21364032, 0.78635968],\n",
       "       [0.18162208, 0.81837792],\n",
       "       [0.14121177, 0.85878823],\n",
       "       [0.182626  , 0.817374  ],\n",
       "       [0.14855955, 0.85144045],\n",
       "       [0.14393308, 0.85606692],\n",
       "       [0.14608209, 0.85391791],\n",
       "       [0.16182643, 0.83817357],\n",
       "       [0.14792764, 0.85207236],\n",
       "       [0.18222171, 0.81777829],\n",
       "       [0.17452562, 0.82547438],\n",
       "       [0.19016547, 0.80983453]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.decomposition\n",
    "#参数n_components为降维后的维数\n",
    "sklearn.decomposition.LatentDirichletAllocation(n_components=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
